{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2606a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#Download the word embeddings from: http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#load embedding file\n",
    "embedding_dim=100\n",
    "path_to_glove_file = \"embeddings/glove.6B/glove.6B.100d.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0f90507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "created game_vocab_list using 10000 words.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "game_vocab_list=[]\n",
    "game_vocab_len=10000\n",
    "with open(path_to_glove_file) as f:\n",
    "    i=0\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        if i<game_vocab_len:\n",
    "            game_vocab_list.append(word)\n",
    "        i+=1\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "print(\"created game_vocab_list using\", len(game_vocab_list), \"words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "46d48722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 0 (10000, 100)\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords when creating vocab list\n",
    "# test_vocab_list=['apple', 'orange', 'tree', 'machine', 'rotor', 'elephant', 'jump', 'run', 'laugh', 'misery','glass']\n",
    "vocab_list=game_vocab_list\n",
    "search_matrix = np.zeros((len(vocab_list), embedding_dim))\n",
    "hits=0\n",
    "misses=0\n",
    "for i,word in enumerate(vocab_list):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        search_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "        \n",
    "\n",
    "        \n",
    "print(hits, misses, np.shape(search_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "90d91219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "dog\n",
      "dog\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_matrix)\n",
    "#lower case\n",
    "#check if in embeddings_index\n",
    "def euclidean_distances(entered_word):\n",
    "    #entered_word can be any word in the embedding, not just the test_vocab_list\n",
    "    distances=np.sqrt(np.sum((search_matrix-embeddings_index[entered_word])**2,axis=1))\n",
    "    return distances\n",
    "\n",
    "def cosine_similarity(entered_word):\n",
    "    #entered_word can be any word in the embedding, not just the test_vocab_list\n",
    "    numerator=np.dot(search_matrix,embeddings_index[entered_word])\n",
    "    denominator=np.sqrt(np.linalg.norm(search_matrix,axis=1))*np.sqrt(np.linalg.norm(embeddings_index[entered_word]))\n",
    "    return numerator/denominator\n",
    "\n",
    "def custom_distance(entered_word):\n",
    "    return euclidean_distances(entered_word)-cosine_similarity(entered_word)\n",
    "\n",
    "check_word='canine'\n",
    "# print(euclidean_distances(check_word))\n",
    "print(vocab_list[np.argmin(euclidean_distances(check_word))])\n",
    "\n",
    "# print(cosine_similarity(check_word))\n",
    "print(vocab_list[np.argmax(cosine_similarity(check_word))])\n",
    "# print(cosine_similarity(check_word)/euclidean_distances(check_word))\n",
    "print(vocab_list[np.argmin(custom_distance(check_word))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "3744c14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.033321043872665\n",
      "cat\n"
     ]
    }
   ],
   "source": [
    "#let's use euclidean distance, because that's easiest to see how it will work with 3 points.\n",
    "\n",
    "#given 2 words, find the word closest to both of them\n",
    "\n",
    "word1='toy'\n",
    "word2='tree'\n",
    "\n",
    "def combined_distances(word1,word2):\n",
    "    distance_to_w1=euclidean_distances(word1)\n",
    "    distance_to_w2=euclidean_distances(word2)\n",
    "    combined_distances=np.sqrt(distance_to_w1**2+distance_to_w2**2)\n",
    "    #can't pick the word1 or word2. Easier to set to large value than delete so I don't mess up indicies.\n",
    "    combined_distances[np.argwhere(distance_to_w1==0)]+=10**10\n",
    "    combined_distances[np.argwhere(distance_to_w2==0)]+=10**10\n",
    "    return combined_distances\n",
    "\n",
    "def middle_word(word1,word2):\n",
    "    distances=combined_distances(word1,word2)\n",
    "    return(game_vocab_list[np.argmin(distances)])\n",
    "\n",
    "def distance_to_word(check_word, word1,word2):\n",
    "    distances=combined_distances(word1,word2)\n",
    "    index = game_vocab_list.index(check_word)\n",
    "    return(distances[index])\n",
    "\n",
    "print(distance_to_word('baseball', word1, word2))\n",
    "print(middle_word(word1,word2))\n",
    "\n",
    "# game_vocab_list[np.argmin(combined_distances)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81e2db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_env]",
   "language": "python",
   "name": "conda-env-tf_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
