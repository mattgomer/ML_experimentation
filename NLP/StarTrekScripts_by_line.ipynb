{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7698a984-6f4d-4a47-b69c-6f9121a030b3",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this project, I want to generate star trek scripts. Previously I tried to train locally to predict the next word, but ran into hardware issues. Now I am going to try instruction fine-tuning and Parameter Efficient Fine-tuning. I also think it might help to try to generate sequences (lines of dialog) rather than single word, since I can transfer that skill from other transformer models.\n",
    "\n",
    "This time, let's try to use the FLAN-T5 model as our tokenizer and base model. We're now using Pytorch instead of tensorflow because I can adapt code to easily load the HuggingFace FLAN-T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca164953-ad9a-4288-84fd-051f579f1c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "# from transformers import TFGPT2LMHeadModel, TFGPT2Model, GPT2Tokenizer, GPT2Config\n",
    "# from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51f1795b-d0e4-4e18-892a-c5fbcf8f1866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Star Trek Transcripts - Where No Man Has Gone Before\n",
      "\n",
      "\n",
      "Where\n",
      "No Man Has Gone Before\n",
      "Stardate:\n",
      "1312.4\n",
      "Original Airdate: 22 Sep, 1966\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "Captain's log, Star date 1312.4. The impossible has happened. From\n",
      "directly ahead, we're picking up a recorded distress signal, the call\n",
      "letters of a vessel which has been missing for over two centuries. Did\n",
      "another Earth ship once probe out of the galaxy as we intend to do?\n",
      "What happened to it out there? Is this some warning they've left\n",
      "behind?\n",
      "\n",
      "[Briefing room]\n",
      "\n",
      "SPOCK: Your move, Captain. \n",
      "KIRK: We should have intercepted by now. The Bridge said they'd call. \n",
      "SPOCK: I'll have you checkmated your next move. \n",
      "KIRK: Have I ever mentioned you play a very irritating game of chess,\n",
      "Mister Spock? \n",
      "SPOCK: Irritating? Ah, yes. One of your Earth emotions. \n",
      "KIRK: Certain you don't know what irritation is? \n",
      "SPOCK: The fact one of my ancestors married a human female \n",
      "KIRK: Terrible having bad blood like that. \n",
      "KELSO [on monitor] Bridge to briefing lounge. Object is now within\n",
      "tractor beam range. \n",
      "KIRK: No visual contact, Mister Kelso? \n",
      "KELSO [on monitor]: No, sir. It's too small to be a vessel. It only\n",
      "reads about one metre in diameter. \n",
      "SPOCK: Not large enough even for a lifeboat. \n",
      "KELSO [on monitor] Small enough to bring it aboard, sir, if you want to\n",
      "risk it. \n",
      "KIRK: Lock onto it, Mister Kelso.\n",
      "\n",
      "[Transporter room]\n",
      "\n",
      "SCOTT: Materialiser ready, sir. \n",
      "KIRK: Bring it aboard. Old-style ship recorder that could be ejected\n",
      "when something threatened the ship. \n",
      "SPOCK: More like destroyed the ship in this case. Look at it. Burnt,\n",
      "pitted. \n",
      "KIRK: Let's hope its tapes are intact. We'll feed it through Mister\n",
      "Spock's computer. \n",
      "SCOTT: Yes, sir. It's begun transmitting, sir. \n",
      "KIRK: Flash the Bridge. Put all decks on the alert.\n",
      "\n",
      "[Turbolift]\n",
      "\n",
      "MITCHELL: Hold it, Jim. \n",
      "KIRK: Getting into shape? \n",
      "MITCHELL: Yeah, well I figured you weren't on the Bridge. Kelso's voice\n",
      "sounded a little nervous. Well, uh, you fi\n"
     ]
    }
   ],
   "source": [
    "f = open(\"../data/StarTrek_scripts/all_scripts_raw.json\")\n",
    "json_file = json.load(f)\n",
    "f.close()\n",
    "#start with TOS: might be more manageable\n",
    "TOS_scripts=json_file['TOS']\n",
    "print(TOS_scripts['episode 3'][:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467cff7-cb68-4c50-8a7c-ff2317c1888c",
   "metadata": {},
   "source": [
    "### General plan\n",
    "\n",
    "I want to generate a star trek script. The input is one line of dialog (or stage direction), the output is the next line. Since we're trying to instruction-tune FLAN-T5, I don't want to introduce custom tokens. Hopefully it figures out what is dialog and what is stage direction.\n",
    "\n",
    "To make this, that means I need to:\n",
    "- parse the scripts, separating by line\n",
    "\n",
    "- Remove episode title at beginning and copyright stuff at the end.\n",
    "    \n",
    "- Create segments of line, next line pairs\n",
    "\n",
    "- Embed the wordings\n",
    "\n",
    "- Split train/test data\n",
    "\n",
    "- Create model and train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "329d7e53-c176-4f54-99ab-f6505102018f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain's log, Stardate 1513.1. Our position, orbiting planet M-113. On board the Enterprise, Mister Spock temporarily in command. On the planet the ruins of an ancient and long-dead civilisation. Ship's surgeon McCoy and myself are now beaming down to the planet's surface. Our mission, routine medical examination of archaeologist Robert Crater and his wife Nancy. Routine but for the fact that Nancy Crater is that one woman in Doctor McCoy's past.\n",
      "\n",
      "\n",
      "[Planet surface outside building]\n",
      "\n",
      "\n",
      "KIRK: Shall we pick some flowers, Doctor? When a man visits an old girlfriend she usually expects something like that.\n",
      "\n",
      "\n",
      "MCCOY: Is that how you get girls to like you, by bribing them? There doesn't seem to be anybody around, does there.\n",
      "\n",
      "\n",
      "KIRK: They'll be along. You rushed us down ten minutes early. (enter building)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#functions to remove metadata and split script\n",
    "\n",
    "def remove_metadata(script):\n",
    "    # Find the position of the 17th newline character\n",
    "    start_pos = -1\n",
    "    for _ in range(17):\n",
    "        start_pos = script.find('\\n', start_pos + 1)\n",
    "        \n",
    "    # Slice the string from the character after the next newline\n",
    "    if start_pos != -1:\n",
    "        script = script[start_pos + 1:]\n",
    "    \n",
    "    # Find the position of \"<Back\"\n",
    "    pos = script.find(\"<Back\")\n",
    "\n",
    "    # If found, cut off everything past that point\n",
    "    if pos != -1:\n",
    "        script = script[:pos]\n",
    "    return script\n",
    "\n",
    "def parse_lines(script_string):\n",
    "    #replace \\r\\n with spaces\n",
    "    # script_string=script_string.replace(\"\\r\\n\", \" \")\n",
    "    \n",
    "    # Regular expression with positive lookahead to match \"\\n[A-Z]+:\"\n",
    "    pattern = r\"(?=\\n[A-Z]+:|\\[[^\\]]+\\])\"\n",
    "    \n",
    "    # Use re.split to break the string at each match\n",
    "    split_script = re.split(pattern, script_string)\n",
    "    parsed_script = [clean_single_line(line) for line in split_script]\n",
    "    return parsed_script\n",
    "\n",
    "def clean_single_line(line):\n",
    "    line=line.replace(\"\\r\\n\", \" \")\n",
    "    line=line.replace(\"\\r\", \"\")\n",
    "    line=line.replace(\"\\n\", \"\")\n",
    "    return line.strip()\n",
    "\n",
    "def recombine_lines(script_lines):\n",
    "    return \"\\n\".join(script_lines)\n",
    "\n",
    "i=1\n",
    "parsed_script=parse_lines(remove_metadata(TOS_scripts['episode '+str(i)]))\n",
    "for j in range(5):\n",
    "    print(parsed_script[j])\n",
    "    print('\\n')\n",
    "# print(recombine_lines(parsed_script))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c537d614-c5d9-42dd-8d35-3fd7873966c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "290555986da04105b5e4420ddba3fe6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f651054e096e436288c245daf5e50407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f378e80624e4bf78787e5c74f3ed6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1621bdb22b2f45b28f791a8ab56fa7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e4d1df42454cfa91f7490d34b23881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2781d3b2dc9940779102af3193516b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d98b5a9ca034527b4541d5e41d2a66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e88b7f58-f0d2-46fc-92e2-41db4d5f0b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Predict the next line of this Star Trek script:\n",
      "\n",
      "Captain's log, Stardate 1513.1. Our position, orbiting planet M-113. On board the Enterprise, Mister Spock temporarily in command. On the planet the ruins of an ancient and long-dead civilisation. Ship's surgeon McCoy and myself are now beaming down to the planet's surface. Our mission, routine medical examination of archaeologist Robert Crater and his wife Nancy. Routine but for the fact that Nancy Crater is that one woman in Doctor McCoy's past.\n",
      "\n",
      "[Planet surface outside building]\n",
      "\n",
      "KIRK: Shall we pick some flowers, Doctor? When a man visits an old girlfriend she usually expects something like that.\n",
      "\n",
      "MCCOY: Is that how you get girls to like you, by bribing them? There doesn't seem to be anybody around, does there.\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "ACTUAL NEXT LINE:\n",
      "KIRK: They'll be along. You rushed us down ten minutes early. (enter building)\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "KIRK: I'm not sure.\n"
     ]
    }
   ],
   "source": [
    "j=0\n",
    "line_1in = parsed_script[j]\n",
    "line_2in = parsed_script[j+1]\n",
    "line_3in = parsed_script[j+2]\n",
    "line_4in = parsed_script[j+3]\n",
    "next_line = parsed_script[j+4]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Predict the next line of this Star Trek script:\n",
    "\n",
    "{line_1in}\n",
    "\n",
    "{line_2in}\n",
    "\n",
    "{line_3in}\n",
    "\n",
    "{line_4in}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"], \n",
    "        max_new_tokens=200,\n",
    "    )[0], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'ACTUAL NEXT LINE:\\n{next_line}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b8673-a4fc-4daa-8d30-7d40f418721a",
   "metadata": {},
   "source": [
    "#### Some thoughts on training and test\n",
    "I want to tokenize all the scripts and make chunks of 80 tokens used to predict the next word. But if I take words 0:8, then 1:81, and so on, the data will be highly correlated. This means I can't just randomly take 20\\% of these chunks out for test data. Instead, I'll split at the episode level. I have 80 episodes- I'll arbitrarily assign 16 episodes to test data, and set those scripts aside.\n",
    "\n",
    "### Process scripts and chunk into training and test sets\n",
    "I initially ran this with a standard layout of 10% validation, 20% test data, but I found that I wasn't really using the test data any different than the validation data. The real test is at the end when I generate something new. Furthermore, I compared two cases: one where it seemed to have its best validation loss (I only trained for ~50 epochs due to hardware limitations), and one where it had the best train loss, but was overfit. The overfit result actually seems to produce more realistic scripts in the final product, so I don't think overfitting as as dangerous for this application as it would be for others. For these reasons, I'm now going to try removing the test set and keeping only 5% in validation, and increasing the size of the training sample as much as I can\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55f7bc6-018d-4d6a-bc67-662495e6680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  30735\n",
      "train shape:  (1368560, 80)\n",
      "val shape:  (71059, 80)\n"
     ]
    }
   ],
   "source": [
    "scripts=[TOS_scripts['episode '+str(i)] for i in range(len(TOS_scripts))]\n",
    "random_state=42\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# train_scripts, test_scripts = train_test_split(scripts, test_size=0.2, random_state=random_state)\n",
    "# train_scripts, val_scripts = train_test_split(train_scripts, test_size=1/8, random_state=random_state)  # 10% of 80% = 1/8\n",
    "train_scripts, val_scripts = train_test_split(scripts, test_size=0.05, random_state=random_state)  # 10% of 80% = 1/8\n",
    "# Initialize a set to hold unique new tokens\n",
    "unique_new_tokens = set()\n",
    "\n",
    "# Preprocess the scripts and collect new tokens\n",
    "processed_scripts = []\n",
    "for script in train_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_scripts.append(processed_text)\n",
    "    unique_new_tokens.update(new_tokens)\n",
    "new_tokens=list(unique_new_tokens)\n",
    "\n",
    "for token in [\"<loc>\", \"<char>\", \"<sd>\", \"<end>\", \"<pad>\"]:\n",
    "    new_tokens.append(token)\n",
    "add_tokens = [token for token in new_tokens if token not in tokenizer.get_vocab()]\n",
    "# Add unique new tokens to the tokenizer\n",
    "tokenizer.add_tokens(list(add_tokens))\n",
    "print('vocab size: ',len(tokenizer))\n",
    "\n",
    "# # Tokenize all the processed scripts\n",
    "# tokenizer.pad_token = \"<PAD>\"\n",
    "tokenized_scripts = [tokenizer.tokenize(script) for script in processed_scripts]\n",
    "\n",
    "maxlen = 80  # Max sequence size\n",
    "def create_chunks(tokenized_scripts, chunk_size=maxlen+1, stride=1,padded_chunks_per_script=10000):\n",
    "    #create overlapping chunks of 80 tokens to predict the next word\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for tokenized_script in tokenized_scripts:\n",
    "        for i in range(0, len(tokenized_script) - chunk_size + 1, stride):\n",
    "            chunk = tokenized_script[i:i + chunk_size]\n",
    "            X.append(chunk[:-1])\n",
    "            y.append([chunk[-1]])\n",
    "        #The user might not provide full 80 words, so lets augment using random padded sequences.\n",
    "        for _ in range(padded_chunks_per_script):\n",
    "            start_index = randint(0, len(tokenized_script) - 2) # -2 to leave room for at least one token\n",
    "            random_length = randint(2, chunk_size - 1) # Choose a random length less than chunk_size\n",
    "            end_index = start_index + random_length\n",
    "            # Select the random chunk\n",
    "            chunk = tokenized_script[start_index:end_index]\n",
    "            # print(chunk)\n",
    "            y.append([chunk[-1]])\n",
    "            # print([chunk[-1]])\n",
    "            X_chunk=chunk[:-1]\n",
    "            # Pad the chunk to the desired length\n",
    "            padding_needed = chunk_size - len(chunk)\n",
    "            # pad_token = tokenizer.pad_token_id # or whatever your padding token is\n",
    "            pads= [\"<pad>\"] * padding_needed\n",
    "            X_chunk= pads + X_chunk\n",
    "            # print(X_chunk)\n",
    "            X.append(X_chunk)\n",
    "    X =np.array([tokenizer.convert_tokens_to_ids(x) for x in X])\n",
    "    y =np.array([tokenizer.convert_tokens_to_ids(_y) for _y in y])\n",
    "    return X,y\n",
    "\n",
    "train_X, train_y=create_chunks(tokenized_scripts)\n",
    "\n",
    "processed_val_scripts=[]\n",
    "for script in val_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_val_scripts.append(processed_text)\n",
    "tokenized_val_scripts = [tokenizer.tokenize(script) for script in processed_val_scripts]\n",
    "val_X, val_y=create_chunks(tokenized_val_scripts)\n",
    "\n",
    "# processed_test_scripts=[]\n",
    "# for script in test_scripts:\n",
    "#     processed_text, new_tokens = preprocess_script(script)\n",
    "#     processed_test_scripts.append(processed_text)\n",
    "# tokenized_test_scripts = [tokenizer.tokenize(script) for script in processed_test_scripts]\n",
    "# test_X, test_y=create_chunks(tokenized_test_scripts\n",
    "print('train shape: ',np.shape(train_X))\n",
    "print('val shape: ',np.shape(val_X))\n",
    "# print('test shape: ',np.shape(test_X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf100f0e-7848-4ba8-88ca-126a152b1af5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<loc>', 'corridor', '<loc>', '<sd>', 'kirk', 'has', 'just', 'climbed', 'up', 'a', 'ladder', 'to', 'this', 'deck', 'when', 'mccoy', 'exits', 'the', 'turbo', '##lift', '.', '<sd>', '<char>', 'mccoy', ':', 'oh', ',', 'captain', '.', 'got', 'a', 'minute', '?', '<char>', 'kirk', ':', 'a', 'minute', '.', '<char>', 'mccoy', ':', 'it', \"'\", 's', 'spock', '.', 'have', 'you', 'noticed', 'anything', 'strange', 'about', 'him', '?', '<char>', 'kirk', ':', 'no', ',', 'nothing', 'in', 'particular', '.', 'why', '?', '<char>', 'mccoy', ':', 'well', ',', 'it', \"'\", 's', 'nothing', 'i', 'can', 'pin', '##point', 'without']\n",
      "<loc> corridor <loc> <sd> kirk has just climbed up a ladder to this deck when mccoy exits the turbolift . <sd> <char> mccoy : oh , captain . got a minute ? <char> kirk : a minute . <char> mccoy : it ' s spock . have you noticed anything strange about him ? <char> kirk : no , nothing in particular . why ? <char> mccoy : well , it ' s nothing i can pinpoint without\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "example_idx=0\n",
    "example=val_X[example_idx]\n",
    "tokens = tokenizer.convert_ids_to_tokens(example)\n",
    "print(tokens)\n",
    "print(' '.join(tokens).replace(' ##', '')) # Joining tokens and handling subtokens\n",
    "print(tokenizer.convert_ids_to_tokens(val_y[example_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f6a3c-bad9-46e3-b56d-7ba2df9808c6",
   "metadata": {},
   "source": [
    "### Define miniature transformer model\n",
    "\n",
    "I'm adapting the small transformer from https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
    "seeing as GPT is way to big for me to train with my hardware.\n",
    "\n",
    "It looks like a big part of my parameters comes from word embeddings and the dense layer at the end, so I might want to try to reduce my vocabulary. I could use TextVectorization instead of BERT's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2868b36a-59dc-4a74-a6e3-b8a601dc8cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build a miniature transformer model based on https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "vocab_size = len(tokenizer)  # Only consider the top 20k words\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    # x = layers.Dense(vocab_size)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x) #added this layer to change the architecture to predict only 1 word\n",
    "    outputs = layers.Dense(vocab_size)(x) #logits of single predicted word\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=loss_fn,\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n",
    "\n",
    "def save_history(history, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "\n",
    "def load_history(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "751a6d25-b4ef-4522-9ea2-687dd6874020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 80, 128)           3944320   \n",
      " ng_2 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_2 (Trans  (None, 80, 128)           593920    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_2  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 30735)             3964815   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8503055 (32.44 MB)\n",
      "Trainable params: 8503055 (32.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "loading checkpoint:  StarTrek_oneword_model_checkpoint3.h5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 10:13:42.855700: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42768/42768 [==============================] - ETA: 0s - loss: 3.5357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 12:10:27.964159: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42768/42768 [==============================] - 7136s 167ms/step - loss: 3.5357 - val_loss: 5.0290\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/miniconda3/envs/tf_metal/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42768/42768 [==============================] - 6767s 158ms/step - loss: 3.3361 - val_loss: 5.0352\n",
      "Epoch 3/10\n",
      "42768/42768 [==============================] - 6531s 153ms/step - loss: 3.2917 - val_loss: 5.0314\n",
      "Epoch 4/10\n",
      "42768/42768 [==============================] - 6582s 154ms/step - loss: 3.2621 - val_loss: 5.1144\n",
      "Epoch 5/10\n",
      "42768/42768 [==============================] - 6594s 154ms/step - loss: 3.2429 - val_loss: 5.0685\n",
      "Epoch 6/10\n",
      "42768/42768 [==============================] - 6685s 156ms/step - loss: 3.2313 - val_loss: 5.1326\n",
      "Epoch 7/10\n",
      "42768/42768 [==============================] - 7003s 164ms/step - loss: 3.2123 - val_loss: 5.1168\n",
      "Epoch 8/10\n",
      "42768/42768 [==============================] - 6953s 163ms/step - loss: 3.1945 - val_loss: 5.1294\n",
      "Epoch 9/10\n",
      "42768/42768 [==============================] - 6761s 158ms/step - loss: 3.1821 - val_loss: 5.1395\n",
      "Epoch 10/10\n",
      "42768/42768 [==============================] - 6718s 157ms/step - loss: 3.1715 - val_loss: 5.1633\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 80, 128)           3944320   \n",
      " ng_3 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_3 (Trans  (None, 80, 128)           593920    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_3  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 30735)             3964815   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8503055 (32.44 MB)\n",
      "Trainable params: 8503055 (32.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "loading checkpoint:  StarTrek_oneword_model_checkpoint4.h5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 05:02:34.291500: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42768/42768 [==============================] - ETA: 0s - loss: 3.2619"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 06:50:12.853128: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42768/42768 [==============================] - 6587s 154ms/step - loss: 3.2619 - val_loss: 5.1767\n",
      "Epoch 2/10\n",
      "42768/42768 [==============================] - 6703s 157ms/step - loss: 3.1600 - val_loss: 5.1758\n",
      "Epoch 3/10\n",
      "42768/42768 [==============================] - 6979s 163ms/step - loss: 3.1422 - val_loss: 5.1918\n",
      "Epoch 4/10\n",
      "42768/42768 [==============================] - 7531s 176ms/step - loss: 3.1342 - val_loss: 5.2722\n",
      "Epoch 5/10\n",
      "42768/42768 [==============================] - 7540s 176ms/step - loss: 3.1146 - val_loss: 5.2318\n",
      "Epoch 6/10\n",
      "42768/42768 [==============================] - 6800s 159ms/step - loss: 3.1053 - val_loss: 5.2011\n",
      "Epoch 7/10\n",
      "42768/42768 [==============================] - 7480s 175ms/step - loss: 3.0976 - val_loss: 5.3004\n",
      "Epoch 8/10\n",
      "42768/42768 [==============================] - 7555s 177ms/step - loss: 3.0975 - val_loss: 5.2758\n",
      "Epoch 9/10\n",
      "42768/42768 [==============================] - 7585s 177ms/step - loss: 3.0878 - val_loss: 5.2959\n",
      "Epoch 10/10\n",
      "42768/42768 [==============================] - 7061s 165ms/step - loss: 3.0748 - val_loss: 5.3313\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "for checkpoint_load_idx in range(3,5):\n",
    "    # Load previous history if exists\n",
    "    try:\n",
    "        history_data = load_history(\"StarTrek_oneword_history.json\")\n",
    "    except:\n",
    "        history_data = {'loss': [], 'val_loss': []}\n",
    "\n",
    "    # Load model if exists\n",
    "    model = create_model()  # Your function to create a model\n",
    "    model.summary()\n",
    "    try:\n",
    "        model.load_weights(\"StarTrek_oneword_model_checkpoint\"+str(checkpoint_load_idx)+\".h5\")\n",
    "        checkpoint_cb = ModelCheckpoint(\"StarTrek_oneword_model_checkpoint\"+str(checkpoint_load_idx+1)+\".h5\",\n",
    "                                        save_best_only=True, monitor='loss')\n",
    "        print('loading checkpoint: ', \"StarTrek_oneword_model_checkpoint\"+str(checkpoint_load_idx)+\".h5\" )\n",
    "    except:\n",
    "\n",
    "        checkpoint_cb = ModelCheckpoint(\"StarTrek_oneword_model_checkpoint\"+str(checkpoint_load_idx)+\".h5\",\n",
    "                                        save_best_only=True, monitor='loss')\n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        train_X, train_y, \n",
    "        validation_data=(val_X, val_y), \n",
    "        epochs=10, \n",
    "        callbacks=[checkpoint_cb]\n",
    "    )\n",
    "\n",
    "    # Update history\n",
    "    for key in history.history:\n",
    "        history_data[key].extend(history.history[key])\n",
    "\n",
    "    # Save history\n",
    "    save_history(history, \"StarTrek_oneword_history.json\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open('StarTrek_oneword_history.json', 'r') as f:\n",
    "#     loaded_history = json.load(f)\n",
    "\n",
    "# for loop_i in range(10):\n",
    "#     checkpoint_filepath = 'StarTrek_oneword_model_checkpoint'+str(loop_i+1)+'.h5'\n",
    "\n",
    "#     model_checkpoint_callback = ModelCheckpoint(\n",
    "#         filepath=checkpoint_filepath,\n",
    "#         save_weights_only=True,\n",
    "#         monitor='loss',\n",
    "#         mode='min',\n",
    "#         save_best_only=True)\n",
    "#     model.load_weights('StarTrek_oneword_model_checkpoint'+str(loop_i)+'.h5')\n",
    "#     history=model.fit(train_X, \n",
    "#                   train_y, \n",
    "#                   validation_data=(val_X, val_y), \n",
    "#                   callbacks=[model_checkpoint_callback], \n",
    "#                   verbose=1, \n",
    "#                   epochs=10)\n",
    "#     hist_dict=history.history\n",
    "#     for key in hist_dict.keys():\n",
    "#         loaded_history[key].extend(hist_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e057d-1d4a-40e8-a4b4-12b7cf341ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = create_model()\n",
    "# model.load_weights('StarTrek_oneword_model_checkpoint8.h5')\n",
    "# loss = model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2c3e417-5be7-4018-ac12-07d61685db24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot history\n",
    "# print(history.history)\n",
    "loaded_history=load_history(\"StarTrek_oneword_history.json\")\n",
    "loss = loaded_history['loss']\n",
    "val_loss = loaded_history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2efd7cf0-1168-4fd5-a92e-f93cee6294d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 80, 128)           3944320   \n",
      " ng_4 (TokenAndPositionEmbe                                      \n",
      " dding)                                                          \n",
      "                                                                 \n",
      " transformer_block_4 (Trans  (None, 80, 128)           593920    \n",
      " formerBlock)                                                    \n",
      "                                                                 \n",
      " global_average_pooling1d_4  (None, 128)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 30735)             3964815   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8503055 (32.44 MB)\n",
      "Trainable params: 8503055 (32.44 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "[ 1996 15386 18412  1012 30732 30731 16075  1024  2821  1010  2952  1012\n",
      "  2288  1037  3371  1029 30731 11332  1024  1037  3371  1012 30731 16075\n",
      "  1024  2009  1005  1055 30648  1012  2031  2017  4384  2505  4326  2055\n",
      "  2032  1029 30731 11332  1024  2053  1010  2498  1999  3327  1012  2339\n",
      "  1029 30731 16075  1024  2092  1010  2009  1005  1055  2498  1045  2064\n",
      "  9231  8400  2302  2019  7749  1010  2021  2002  1005  1055  2468  6233\n",
      "  2717  3512  1012  2065  2002  2020  2025  1037]\n",
      "['<loc>']\n",
      "['the', 'turbo', '##lift', '.', '<sd>', '<char>', 'mccoy', ':', 'oh', ',', 'captain', '.', 'got', 'a', 'minute', '?', '<char>', 'kirk', ':', 'a', 'minute', '.', '<char>', 'mccoy', ':', 'it', \"'\", 's', 'spock', '.', 'have', 'you', 'noticed', 'anything', 'strange', 'about', 'him', '?', '<char>', 'kirk', ':', 'no', ',', 'nothing', 'in', 'particular', '.', 'why', '?', '<char>', 'mccoy', ':', 'well', ',', 'it', \"'\", 's', 'nothing', 'i', 'can', 'pin', '##point', 'without', 'an', 'examination', ',', 'but', 'he', \"'\", 's', 'become', 'increasingly', 'rest', '##ive', '.', 'if', 'he', 'were', 'not', 'a']\n",
      "input = \n",
      " the turbolift . <sd> <char> mccoy : oh , captain . got a minute ? <char> kirk : a minute . <char> mccoy : it ' s spock . have you noticed anything strange about him ? <char> kirk : no , nothing in particular . why ? <char> mccoy : well , it ' s nothing i can pinpoint without an examination , but he ' s become increasingly restive . if he were not a \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-02 09:03:57.865371: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 436ms/step\n",
      "\n",
      "\n",
      "generated = \n",
      " the turbolift . <sd> <char> mccoy : oh , captain . got a minute ? <char> kirk : a minute . <char> mccoy : it ' s spock . have you noticed anything strange about him ? <char> kirk : no , nothing in particular . why ? <char> mccoy : well , it ' s nothing i can pinpoint without an examination , but he ' s become increasingly restive . if he were not a little\n"
     ]
    }
   ],
   "source": [
    "def convert_input_to_token_idx(string):\n",
    "    processed_string=preprocess_script(string)[0][:-5]\n",
    "    input_tokens= tokenizer.tokenize(processed_string)\n",
    "    input_tokens=tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    padding_needed = 80 - len(input_tokens)\n",
    "    pad_token = tokenizer.pad_token_id # or whatever your padding token is\n",
    "    input_tokens += [pad_token] * padding_needed\n",
    "    return np.array(input_tokens)\n",
    "\n",
    "def convert_token_idx_to_string(tokens):\n",
    "    string_array=tokenizer.convert_ids_to_tokens(tokens)\n",
    "    string_array=[string for string in string_array if string != \"[PAD]\"]\n",
    "    text = ' '.join(string_array).replace(' ##', '').replace('[PAD]', '')\n",
    "    return text\n",
    "\n",
    "model = create_model()  # Your function to create a model\n",
    "model.summary()\n",
    "model.load_weights(\"StarTrek_oneword_model_checkpoint3.h5\")\n",
    "\n",
    "input_string=\"Captain's log, star\"\n",
    "# input_string=\"\\nKIRK: Set ship status condition\"\n",
    "input_tokens=convert_input_to_token_idx(input_string)\n",
    "input_tokens=val_X[17]\n",
    "print(input_tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(train_y[8]))\n",
    "# print(input_tokens)\n",
    "# print(type(train_X[0]))\n",
    "# print(np.shape(train_X[0]))\n",
    "print(tokenizer.convert_ids_to_tokens(input_tokens))\n",
    "print('input = \\n',convert_token_idx_to_string(input_tokens),'\\n')\n",
    "generated_token=np.argmax(model.predict(np.expand_dims(input_tokens,axis=0)),axis=-1)\n",
    "next_word=convert_token_idx_to_string(generated_token)\n",
    "print('\\n')\n",
    "print('generated = \\n', convert_token_idx_to_string(input_tokens)+' '+next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba347ac2-8269-4047-b5da-238651708068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mccoy has given me his medical evaluation of your condition . he says you ' re going to die unless something is done . what ? is it something only your planet can do for you ? spock ! you ' ve been called the best first officer in the fleet . that ' s an enormous asset to me . if i have to lose that first officer , i want to know why . \n",
      " spock : it ' s not a vulcan . \n",
      " kirk : i ' m sure that ' s what i am going to be a very good . i ' m sure that ' s what i ' ve got to say that thing ? \n",
      " kirk : yes , i know what you ' re going to do . \n",
      " mccoy : i ' m sure that ' s what i ' ve got a few hours . \n",
      " kirk : i ' m afraid that ' s what i want to know . \n",
      " kirk : i ' m afraid that ' s what you comm n . \n",
      " kirk : i ' m afraid you ' ll be a very well , captain . i ' m afraid of you . i ' m afraid of that , i ' m afraid of you . i ' m afraid you ' ll be a very well , captain . i ' m not sure you ' ll have to take you to the enterprise . \n",
      " kirk : i ' ll be right to be a very good . i ' ll take you to the enterprise . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#generate a longer script\n",
    "\n",
    "input_tokens=val_X[2000]\n",
    "script=list(input_tokens)\n",
    "for i in range(200):\n",
    "    generated_token=list(np.argmax(model.predict(np.expand_dims(input_tokens,axis=0),verbose=0),axis=-1))\n",
    "    script.append(generated_token[0])\n",
    "    input_tokens=script[-80:]\n",
    "script=convert_token_idx_to_string(script)\n",
    "script = script.replace('<char>', '\\n')\n",
    "print(script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbbe947-1462-47af-be85-29ac2e3cb701",
   "metadata": {},
   "source": [
    "Not as humanlike as I hoped. I think transformers need a lot more data to sound humanlike. Maybe if I trained starting from GPT, but my hardware limits me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f40d6-8556-4b1b-8845-74ca7fb35578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experimenting using TextVectorization tokenization instead of DistilBERT\n",
    "# scripts=[TOS_scripts['episode '+str(i)] for i in range(len(TOS_scripts))]\n",
    "# random_state=42\n",
    "# train_scripts, test_scripts = train_test_split(scripts, test_size=0.2, random_state=random_state)\n",
    "# train_scripts, val_scripts = train_test_split(train_scripts, test_size=1/8, random_state=random_state)  # 10% of 80% = 1/8\n",
    "\n",
    "# processed_train_scripts = [preprocess_script(script)[0] for script in train_scripts]\n",
    "# big_string = ' '.join(processed_train_scripts)\n",
    "\n",
    "# # Convert to a TensorFlow Dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices([big_string])\n",
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     cleaned_string = tf.strings.regex_replace(stripped_html, \"\\xa0\", \" \")  # Replacing non-breaking space with regular space\n",
    "#     # Exclude < and > from the punctuation string\n",
    "#     punctuation_without_angle_brackets = string.punctuation.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "#     return tf.strings.regex_replace(cleaned_string, f\"([{punctuation_without_angle_brackets}])\", r\" \\1\")\n",
    "\n",
    "# # Create a vectorization layer and adapt it to the text\n",
    "# vectorize_layer = TextVectorization(\n",
    "#     standardize=custom_standardization,\n",
    "#     max_tokens=vocab_size - 1,\n",
    "#     output_mode=\"int\",\n",
    "#     # output_sequence_length=maxlen + 1,\n",
    "# )\n",
    "# with tf.device('/CPU:0'):\n",
    "#     # Your code here (e.g., adapting the vectorize layer)\n",
    "#     vectorize_layer.adapt(dataset) #for some reason this won't work on my GPU\n",
    "# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "# #create chunks of training, val, and test data\n",
    "# window_size = 81\n",
    "# step_size = 3  # This defines the overlap; change as needed\n",
    "# padded_chunks_per_script=0 #not yet implemented\n",
    "\n",
    "# # Tokenize all the processed scripts\n",
    "# processed_train_scripts_tensor = tf.constant(processed_train_scripts)\n",
    "# vectorized_train_scripts = vectorize_layer(processed_train_scripts_tensor)\n",
    "# def create_chunks(tokenized_scripts, window_size=window_size, step_size=step_size, padded_chunks_per_script=padded_chunks_per_script):\n",
    "#     chunks = []\n",
    "#     for script in tokenized_scripts:\n",
    "#         for i in range(0, len(script) - window_size + 1, step_size):\n",
    "#             chunk = script[i:i + window_size]\n",
    "#             chunks.append(chunk)\n",
    "#     return chunks\n",
    "\n",
    "# train_chunks=create_chunks(vectorized_train_scripts)\n",
    "# train_X=np.array([chunk[:-1] for chunk in train_chunks])\n",
    "# train_y=np.array([chunk[1:] for chunk in train_chunks])\n",
    "\n",
    "# #do the same for validation and test data\n",
    "# processed_val_scripts = [preprocess_script(script)[0] for script in val_scripts]\n",
    "# processed_val_scripts_tensor = tf.constant(processed_val_scripts)\n",
    "# vectorized_val_scripts = vectorize_layer(processed_val_scripts_tensor)\n",
    "# val_chunks=create_chunks(vectorized_val_scripts)\n",
    "# val_X=np.array([chunk[:-1] for chunk in val_chunks])\n",
    "# val_y=np.array([chunk[1:] for chunk in val_chunks])\n",
    "\n",
    "\n",
    "# processed_test_scripts = [preprocess_script(script)[0] for script in test_scripts]\n",
    "# processed_test_scripts_tensor = tf.constant(processed_test_scripts)\n",
    "# vectorized_test_scripts = vectorize_layer(processed_test_scripts_tensor)\n",
    "# test_chunks=create_chunks(vectorized_test_scripts)\n",
    "# test_X=np.array([chunk[:-1] for chunk in test_chunks])\n",
    "# test_y=np.array([chunk[1:] for chunk in test_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558e055-9c4e-4b77-bc9e-b8146d2ea512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #demonstrate how the vectorize layer works and how the x y vectors work\n",
    "# print(np.shape(train_X))\n",
    "# print(np.shape(val_X))\n",
    "# print(np.shape(test_X))\n",
    "# print(train_X[-1]) #batching caused shorter scripts to be filled with zeros. Maybe go back to the loop mehtod\n",
    "# print([vocab[idx] for idx in val_X[12]])\n",
    "# print([vocab[idx] for idx in val_y[12]])\n",
    "# # print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47205eb0-798c-484e-b2e5-7ddd4ac3abbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# #use GPT-2 model. We'll need to train from here to learn star trek vocab\n",
    "# Initializing a GPT2 configuration\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('distilgpt2')\n",
    "configuration.vocab_size = len(tokenizer)\n",
    "# configuration.n_positions = 128 #The maximum sequence length that this model might ever be used with. \n",
    "# configuration.n_embd = 768, #768, #Dimensionality of the embeddings and hidden states.\n",
    "# configuration.n_layer = 4, #12, #Number of hidden layers in the Transformer encoder.\n",
    "# configuration.n_head = 4, #12, #Number of attention heads for each attention layer in the Transformer encoder.\n",
    "                           \n",
    "model = TFGPT2Model(configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size\n",
    "\n",
    "# If you want to see the summary, you can build the model first:\n",
    "model.build(input_shape=(None, 128))  # None for batch size, 128 for sequence length\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# configuration = GPT2Config(vocab_size = len(tokenizer), \n",
    "#                            n_positions = 128, #The maximum sequence length that this model might ever be used with. \n",
    "#                            n_embd = 64, #768, #Dimensionality of the embeddings and hidden states.\n",
    "#                            n_layer = 4, #12, #Number of hidden layers in the Transformer encoder.\n",
    "#                            n_head = 4, #12, #Number of attention heads for each attention layer in the Transformer encoder.\n",
    "#                            n_inner = None,\n",
    "#                            activation_function = 'gelu_new',\n",
    "#                            resid_pdrop = 0.1,\n",
    "#                            embd_pdrop = 0.1,\n",
    "#                            attn_pdrop = 0.1,\n",
    "#                            layer_norm_epsilon = 1e-05,\n",
    "#                            initializer_range = 0.02,\n",
    "#                            summary_type = 'cls_index',\n",
    "#                            summary_use_proj = True,\n",
    "#                            summary_activation = None,\n",
    "#                            summary_proj_to_labels = True,\n",
    "#                            summary_first_dropout = 0.1,\n",
    "#                            scale_attn_weights = True,\n",
    "#                            use_cache = True,\n",
    "#                            bos_token_id = 50256,\n",
    "#                            eos_token_id = 50256,\n",
    "#                            scale_attn_by_inverse_layer_idx = False,\n",
    "#                            reorder_and_upcast_attn = False)\n",
    "\n",
    "# # Initializing a model (with random weights) from the configuration\n",
    "# model = TFGPT2LMHeadModel(configuration)\n",
    "# # from tensorflow.keras.layers import Input, Dense\n",
    "# # from tensorflow.keras.models import Model\n",
    "# # input_layer = Input(shape=(128,), dtype='int32')\n",
    "# # gpt2_output = gpt2_model(input_layer)\n",
    "# # output_layer = Dense(vocab_size, activation='softmax')(gpt2_output[0][:,-1,:])\n",
    "\n",
    "# # model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# # model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size\n",
    "# # print(len(tokenizer))\n",
    "\n",
    "# # #use DistilBERT model. We'll need to train from here to learn star trek vocab\n",
    "# # model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# # model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size\n",
    "# print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59a5ac-28c4-4188-8973-f9ce7c11a776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #training\n",
    "# # Train only the embedding layer\n",
    "\n",
    "# # Freeze all layers within the transformer\n",
    "# for layer in model.transformer.h:\n",
    "#     layer.trainable = False\n",
    "# # Unfreeze the embedding layer (wte)\n",
    "# model.transformer.wte.trainable = True\n",
    "# # Freeze all layers first\n",
    "# # for layer in model.layers:\n",
    "# #     layer.trainable = False\n",
    "\n",
    "# # # Unfreeze the embedding layer\n",
    "# # model.layers[0].embeddings.trainable = True\n",
    "\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "# model.summary()\n",
    "# history = model.fit(x=train_X, y=train_y, epochs=10, batch_size=32, validation_data=(val_X, val_y))\n",
    "\n",
    "# # # Save the weights\n",
    "# model.save_weights('StarTrek_weights.h5')\n",
    "\n",
    "# # # Unfreeze more layers (optional)\n",
    "# # for layer in model.layers[:n]:  # Unfreeze the first n layers\n",
    "# #     layer.trainable = True\n",
    "# # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', lr=0.0001) # Smaller learning rate\n",
    "# # model.fit(training_data)\n",
    "\n",
    "# # # Unfreeze all layers (optional)\n",
    "# # for layer in model.layers:\n",
    "# #     layer.trainable = True\n",
    "# # model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', lr=0.00001) # Even smaller learning rate\n",
    "# # model.fit(training_data)\n",
    "\n",
    "# # # Save the final model\n",
    "# # model.save('final_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b511c-5a3c-4a96-ab12-dff332cf01d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# plt.plot(train_loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee74234-3418-4357-b018-59f38150918e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
