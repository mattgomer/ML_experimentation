{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7698a984-6f4d-4a47-b69c-6f9121a030b3",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this project, I want to generate star trek scripts. \n",
    "\n",
    "I originally tokenized them using the GPT2Tokenizer and tried to use TFGPT2LMHeadModel, but I found that even if I freeze everything but the embedding layers, this takes 13 hours per epoch to train on my hardware. I didn't really appreciate the size of the GPT model.\n",
    "\n",
    "Now I will simplify a bit. I'll make everything lower case for a smaller vocab and use a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca164953-ad9a-4288-84fd-051f579f1c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f1795b-d0e4-4e18-892a-c5fbcf8f1866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Star Trek Transcripts - The Cage\n",
      "\n",
      "\n",
      "\n",
      "The\n",
      "Cage\n",
      "Unaired\n",
      "pilot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [Bridge]\n",
      "\n",
      "SPOCK: Check the circuit. \n",
      "TYLER: All operating, sir. \n",
      "SPOCK: It can't be the screen then. Definitely something out there,\n",
      "Captain, headed this way. \n",
      "TYLER: It could be these meteorites. \n",
      "ONE: No, it's something else. There's still something out there. \n",
      "TYLER: It's coming at the speed of light, collision course. The\n",
      "meteorite beam has not deflected it, Captain.\n",
      "ONE: Evasive manoeuvres, sir?\n",
      "PIKE: Steady as we go.\n",
      "GARISON: It's a radio wave, sir. We're passing through an old-style\n",
      "distress signal.\n",
      "PIKE: They were keyed to cause interference and attract attention this\n",
      "way.\n",
      "GARISON: A ship in trouble making a forced landing, sir. That's it. No\n",
      "other message.\n",
      "TYLER: I have a fix. It comes from the Talos star group.\n",
      "ONE: We've no ships or Earth colonies that far out.\n",
      "SPOCK: Their call letters check with a survey expedition. SS Columbia.\n",
      "It disappeared in that region approximately eight\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/StarTrek_scripts/all_scripts_raw.json\")\n",
    "json_file = json.load(f)\n",
    "f.close()\n",
    "#start with TOS: might be more manageable\n",
    "TOS_scripts=json_file['TOS']\n",
    "print(TOS_scripts['episode 0'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467cff7-cb68-4c50-8a7c-ff2317c1888c",
   "metadata": {},
   "source": [
    "### General plan\n",
    "\n",
    "I want to generate a star trek script. The model will be some kind of transformer. The input is a series of tokens, I'll start with 128 tokens (needs padded in case the input is shorter). The output is the next word, i.e. input: [The, quick, brown], output: [quick, brown, fox]\n",
    "\n",
    "To make this, that means I need to:\n",
    "- parse the scripts, remove line breaks and things.\n",
    "\n",
    "    - Also need to remove episode title at beginning and copyright stuff at the end.\n",
    "    - Probably should add a character for stage direction, or perhaps if I keep the colons such that kirk: is distinct from kirk.\n",
    "    \n",
    "- Create segments of input tokens in batches of 128\n",
    "\n",
    "- Embed the wordings\n",
    "\n",
    "- Split train/test data\n",
    "\n",
    "- Create model and train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329d7e53-c176-4f54-99ab-f6505102018f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LOC> Bridge <LOC> <CHAR> Spock: Check the circuit. <CHAR> Tyler: All operating, sir. <CHAR> Spock: It can't be the screen then. Definitely something out there, Captain, headed this way. <CHAR> Tyler: It could be these meteorites. <CHAR> One: No, it's something else. There's still something out there. <CHAR> Tyler: It's coming at the speed of light, collision course. The meteorite beam has not deflected it, Captain. <CHAR> One: Evasive manoeuvres, sir? <CHAR> Pike: Steady as we go. <CHAR> Garison: It's a radio wave, sir. We're passing through an old-style distress signal. <CHAR> Pike: They were keyed to cause interference and attract attention this way. <CHAR> Garison: A ship in trouble making a forced landing, sir. That's it. No other message. <CHAR> Tyler: I have a fix. It comes from the Talos star group. <CHAR> One: We've no ships or Earth colonies that far out. <CHAR> Spock: Their call letters check with a survey expedition. SS Columbia. It disappeared in that region approximately \n",
      "{'Old', 'Haskins', 'Garison', 'Pike', 'Geologist', 'One', 'Survivor', 'Vina', 'Boyce', 'Magistrate', 'Colt', 'Pitcairn', 'Spock', 'Tyler', 'Orion', 'Talosian', 'Officer'}\n",
      "Captain's log, Stardate 1513.1. Our position, orbiting planet M-113. On board the Enterprise, Mister Spock temporarily in command. On the planet the ruins of an ancient and long-dead civilisation. Ship's surgeon McCoy and myself are now beaming down to the planet's surface. Our mission, routine medical examination of archaeologist Robert Crater and his wife Nancy. Routine but for the fact that Nancy Crater is that one woman in Doctor McCoy's past. <LOC> Planet surface outside building <LOC> <CHAR> Kirk: Shall we pick some flowers, Doctor? When a man visits an old girlfriend she usually expects something like that. <CHAR> McCoy: Is that how you get girls to like you, by bribing them? There doesn't seem to be anybody around, does there. <CHAR> Kirk: They'll be along. You rushed us down ten minutes early. <SD> enter building <SD> <LOC> Crater's home <LOC> <CHAR> Kirk: Professor Crater? Professor? Mrs. Crater? Nervous, Dr. McCoy? <CHAR> McCoy: Yeah, a little bit, I guess. You see, we walke\n",
      "{'Rand', 'Blonde', 'McCoy', 'Nancy', 'Redshirt', 'Sulu', 'Kirk', 'Crater', 'Uhura', 'Green', 'Blueshirt', 'Crewman', 'Darnell', 'Spock', 'Transporter', 'Security'}\n",
      "Captain's Log, star date 1533.6. Now manoeuvring to come alongside cargo vessel Antares. Its Captain and First officer are beaming over to us with an unusual passenger. <LOC> Transporter room <LOC> <CHAR> Kirk: All right, Chief, begin materialisation. Captain Ramart, I'm Captain Kirk. <CHAR> Ramart: This is my navigator Tom Nellis. <CHAR> Kirk: How do you do? <CHAR> Nellis: How do you do? <CHAR> Ramart: And this is our young castaway Charlie, Charlie Evans. His dossier. <CHAR> Kirk: Mister Evans. We've heard a great deal about you. Welcome aboard. <CHAR> Ramart: Wonderful boy, Charlie. Its been an honour having him aboard. <CHAR> Nellis: Why, it's been a great pleasure. The things that he's learned in the last <CHAR> Ramart: Absolutely. To think this boy spent practically his whole life alone on that planet. Everyone killed, just a few microtapes to learn from. <CHAR> Charlie: How many humans like me on this ship? <CHAR> Ramart: Like a whole city in space, Charlie. Over four hundred in\n",
      "{'Thasian', 'Rand', 'McCoy', 'Kirk', 'Uhura', 'Navigator', 'Nellis', 'Spock', 'Pilot', 'Ramart', 'Tina', 'Charlie', 'Sam'}\n",
      "Captain's log, Star date 1312.4. The impossible has happened. From directly ahead, we're picking up a recorded distress signal, the call letters of a vessel which has been missing for over two centuries. Did another Earth ship once probe out of the galaxy as we intend to do? What happened to it out there? Is this some warning they've left behind? <LOC> Briefing room <LOC> <CHAR> Spock: Your move, Captain. <CHAR> Kirk: We should have intercepted by now. The Bridge said they'd call. <CHAR> Spock: I'll have you checkmated your next move. <CHAR> Kirk: Have I ever mentioned you play a very irritating game of chess, Mister Spock? <CHAR> Spock: Irritating? Ah, yes. One of your Earth emotions. <CHAR> Kirk: Certain you don't know what irritation is? <CHAR> Spock: The fact one of my ancestors married a human female <CHAR> Kirk: Terrible having bad blood like that. KELSO <LOC> on monitor <LOC> Bridge to briefing lounge. Object is now within tractor beam range. <CHAR> Kirk: No visual contact, Mist\n",
      "{'Dehner', 'Mitchell', 'Smith', 'Sulu', 'Scott', 'Kirk', 'Alden', 'Spock', 'Kelso', 'Piper'}\n",
      "Captain's Log. Our position, orbiting Psi 2000, an ancient world, now a frozen wasteland, about to rip apart in its death throes. Our mission, pick up a scientific party below, observe the disintegration of the planet. <LOC> Psi 2000 control room <LOC> <CHAR> Spock: Check out the life-support systems. <CHAR> Tormolen: Right, sir. All life systems were off, sir. <CHAR> Spock: Someone strangled this woman. TORMOLEN The other four are back there. <CHAR> Spock: Dead? <CHAR> Tormolen: Right, sir. <CHAR> Spock: Engineer at his post? TORMOLEN He's frozen there like he didn't care. <CHAR> Spock: The rest? <CHAR> Tormolen: Well, better look for yourself, Mister Spock. One man was taking a shower fully clothed. <SD> they split up, he takes off his glove to scratch his nose then puts it on the console. Something orange jumps onto his skin. Feeling the cold he puts the glove back on <SD> <CHAR> Spock: Be certain we expose ourselves to nothing. Spock here. Do you read, Enterprise? KIRK <LOC> OC <LO\n",
      "{'Rand', 'McCoy', 'Sulu', 'Scott', 'Kirk', 'Uhura', 'Tormolen', 'Chapel', 'Spock', 'Brent', 'Riley', 'Moody'}\n",
      "<LOC> Planet surface <LOC> <CHAR> Kirk: That should make a good specimen. <CHAR> Sulu: <SD> holding pink animal with a horn <SD> Temperature's starting to drop. <CHAR> Kirk: Yeah. At night it gets down to a hundred and twenty degrees below zero. <CHAR> Sulu: That's nippy.  <CHAR> Fisher: Hey! <CHAR> Kirk: What happened? <CHAR> Fisher: I fell off that bank, sir. Cut my hand. <CHAR> Kirk: Let's see it. Get back to the ship. Report to the Sickbay. <CHAR> Fisher: Yes, sir. Geological Technician Fisher. Ready to beam up. <LOC> Transporter room <LOC> <CHAR> Scott: Right. Locked onto you. Energise. Coadjutor engagement. <CHAR> Wilson: What happened? <CHAR> Fisher: I took a flop. <CHAR> Wilson: Onto what? <CHAR> Fisher: I don't know. Some kind of yellow ore. <CHAR> Scott: Magnetic. Decontaminate that uniform. <CHAR> Fisher: Yes, sir. <SD> ;eaves <SD> <CHAR> Scott: That acted like a burnout. KIRK <LOC> OC <LOC>: Captain Kirk ready to beam up. <CHAR> Scott: Just one moment, Captain. It checks ou\n",
      "{'Farrell', 'Other', 'Rand', 'McCoy', 'Sulu', 'Kirk', 'Scott', 'Spock', 'Fisher', 'Wilson'}\n",
      "Captain's log, Stardate 1329.8. The U.S.S. Enterprise in pursuit of an unidentified vessel. <LOC> Bridge <LOC> <CHAR> Sulu: There he is, sir. Centre screen. Still trying to run away from us, sir. <CHAR> Kirk: Don't lose him, Mister Sulu. <CHAR> Sulu: No, sir. <CHAR> Kirk: Earth ship, Mister Spock? <CHAR> Spock: Difficult to say, Captain. We're getting no registration beam from it. <CHAR> Scott: If it is, he'll soon overload his engines. <CHAR> Spock: Sir, he's pushing his engines too hard. <CHAR> Sulu: Changing course again. He knows we're after him all right. <CHAR> Kirk: Stay with him, Mister Sulu. Communication? <CHAR> Uhura: I've tried all frequencies, sir. He refuses to answer. Unless he's not receiving us. <CHAR> Kirk: He's receiving us alright. <CHAR> Spock: Approaching an asteroid belt, Captain. Schiller rating three five. <CHAR> Kirk: Deflectors on, Mister Farrell. <CHAR> Sulu: He's seen the asteroids too, sir. <CHAR> Kirk: Stay with him. He'll try to lose us in them. <CHAR> S\n",
      "{'Madga', 'Benton', 'Computer', 'Ruth', 'Childress', 'Farrell', 'Gossett', 'Eve', 'Mudd', 'Kirk', 'Scott', 'Uhura', 'Magda', 'Spock', 'Women', 'McCoy', 'Nudd', 'Sulu', 'Security'}\n",
      "<LOC> Bridge <LOC> CREWMAN <LOC> OC <LOC>: All sections, security check in progress. Report. Forward phaser <CHAR> Kirk: We're entering standard orbit, Nurse. It won't be much longer. CREWMAN <LOC> OC <LOC>: Forward scanner to Bridge. <CHAR> Kirk: I understand you gave up a career in bio-research to sign aboard a starship. <CHAR> Chapel: I know he's alive down there, Captain. CREWMAN 2 <LOC> OC <LOC>: Aft scanner to Bridge, Status report, please. Engineering controls <CHAR> Kirk: It's been five years since his last message. <CHAR> Chapel: Roger's a very determined man. He'd find a way to live. <CHAR> Uhura: Beginning signals to surface, sir. <CHAR> Kirk: Put it on all frequencies, Lieutenant. <CHAR> Spock: Ship's record banks show little we don't already know about this planet, Captain. Gravity is one point one of Earth, atmosphere within safety limits. <CHAR> Kirk: But the surface temperature of that planet is one hundred degrees below zero. <CHAR> Spock: It may have been inhabited on\n",
      "{'Kirk', 'Uhura', 'Korby', 'Ruk', 'Chapel', 'Spock', 'Matthews', 'Rayburn', 'Brown', 'Andrea'}\n",
      "<LOC> Bridge <LOC> <CHAR> Kirk: Earth-style distress signal. SOS. <CHAR> Farrell: I've answered it on all frequencies, sir. They don't reply. <CHAR> Spock: Not a vessel, a ground source. The third planet in this solar system, according to my instruments. <CHAR> Farrell: Directly ahead. Definitely an Earth-style signal. <CHAR> Kirk: We're hundreds of light years from Earth, Mister Spock. No colonies or vessels out this far. <CHAR> Spock: Measuring the planet now, Captain. It's spheroid-shaped, circumference twenty four thousand eight hundred seventy four miles. Mass six times ten to the twenty first power tons. Mean density five point five one seven. Atmosphere oxygen, nitrogen. <CHAR> Rand: Earth! <CHAR> Kirk: Not the Earth, another Earth. Another Earth? Captain's Log, stardate 2713.5. In the distant reaches of our galaxy, we have made an astonishing discovery. Earth type radio signals coming from a planet which apparently is an exact duplicate of the Earth. It seems impossible, but th\n",
      "{'Farrell', 'Children', 'Rand', 'Masked', 'Blonde', 'McCoy', 'Kirk', 'Teenager', 'Jahn', 'Red', 'Spock', 'Galloway', 'Miri'}\n",
      "<LOC> Transporter room <LOC> <CHAR> Berkeley: Ready to beam down. Energise. Energise. <CHAR> Kirk: Having trouble, gentlemen? <CHAR> Berkeley: I just don't understand the problem, sir. <CHAR> Kirk: You're beaming cargo down to a penal colony, Mister Berkeley. <CHAR> Berkeley: Their security force field, sir. <CHAR> Kirk: U.S.S. Enterprise to Tantalus colony. WOMAN <LOC> OC <LOC>: Rehab colony. Come in. <CHAR> Kirk: Request opening in your force field for beaming down of cargo. WOMAN <LOC> OC <LOC>: Enterprise, affirmative. Our security cover is now open. <CHAR> Kirk: Energise. Any incoming cargo? <CHAR> Berkeley: Just one item, sir. Some research material bound for the Central Bureau of Penology at Stockholm. WOMAN <LOC> OC <LOC>: Tantalus cargo ready to beam up. <CHAR> Kirk: Oh, Mister Berkeley, you might refamiliarise yourself with the manual on penal colony procedures. <CHAR> Berkeley: Immediately, sir. <CHAR> Kirk: I think you can take the time to lock this up first. <CHAR> Berkele\n",
      "{'Lethe', 'Noel', 'McCoy', 'Kirk', 'Uhura', 'Spock', 'Security', 'Therapist', 'Gelder', 'Berkeley', 'Adams'}\n"
     ]
    }
   ],
   "source": [
    "#functions to remove metadata and add stage direction tokens to script\n",
    "\n",
    "def add_special_tokens(script):\n",
    "    # Replace character names\n",
    "    script = re.sub(r'\\n([A-Z ]+):', r' <CHAR> \\1:', script) #adds <CHAR> token any time theres a new line followed by \"<CAPITALLETTERS>:\"\n",
    "    # You can add more substitutions here for stage directions or other special tokens\n",
    "    script = re.sub(r'[\\[\\{]([^\\]\\}]+)[\\]\\}]', r' <LOC> \\1 <LOC>', script) #add <LOC> token to indicate location\n",
    "    script = re.sub(r'\\(([^)]+)\\)', r' <SD> \\1 <SD>', script)\n",
    "    return script\n",
    "\n",
    "def remove_metadata(script):\n",
    "    # Find the position of the 17th newline character\n",
    "    start_pos = -1\n",
    "    for _ in range(17):\n",
    "        start_pos = script.find('\\n', start_pos + 1)\n",
    "        \n",
    "    # Slice the string from the character after the 8th newline\n",
    "    if start_pos != -1:\n",
    "        script = script[start_pos + 1:]\n",
    "    \n",
    "    # Find the position of \"<Back\"\n",
    "    pos = script.find(\"<Back\")\n",
    "\n",
    "    # If found, cut off everything past that point\n",
    "    if pos != -1:\n",
    "        script = script[:pos]\n",
    "    return script\n",
    "\n",
    "def process_names(text):\n",
    "    unique_names = set()\n",
    "\n",
    "    # Function to replace \"<CHAR> NAME:\" with \"<CHAR> Name:\"\n",
    "    def char_replacer(match):\n",
    "        name = match.group(1)\n",
    "        if name.lower() == \"mccoy\": #McCoy needs special treatment due to unique capitalization\n",
    "            name = \"McCoy\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        unique_names.add(f\"{name}\")\n",
    "        return f\"<CHAR> {name}\"\n",
    "    \n",
    "    # Replace names after \"<CHAR>\"\n",
    "    text = re.sub(r'<CHAR>\\s+([A-Z]{2,})', char_replacer, text)\n",
    "    # Function to replace all other instances of unique names\n",
    "    def name_replacer(match):\n",
    "        name = match.group(0)\n",
    "        if name == \"MCCOY\":\n",
    "            return \" McCoy\"\n",
    "        return name.capitalize() if name.upper() in unique_names else name\n",
    "\n",
    "    # Replace all other instances of unique names\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\b', name_replacer, text)\n",
    "\n",
    "    return text, unique_names\n",
    "\n",
    "def preprocess_script(script):\n",
    "    \n",
    "    script=add_special_tokens(remove_metadata(script))\n",
    "    script, names =process_names(script)\n",
    "    script=script.replace('\\n', ' ')\n",
    "    script=script.replace('\\r', ' ')\n",
    "    # Replace multiple spaces with a single space\n",
    "    script = re.sub(' +', ' ', script)\n",
    "    script += \"<END>\"\n",
    "    script = script.strip()\n",
    "    return script, names\n",
    "\n",
    "for i in range(10):\n",
    "    # print(TOS_scripts['episode '+str(i)][:1100])\n",
    "    \n",
    "    script, names=preprocess_script(TOS_scripts['episode '+str(i)])\n",
    "    print(script[:1000])\n",
    "    print(names)\n",
    "    # print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c537d614-c5d9-42dd-8d35-3fd7873966c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['McCoy', 'Mara', 'Sulu', 'Kirk', 'Uhura', 'Kang', 'Klingon', 'Bald', 'Spock', 'Chekov', '<LOC>', '<CHAR>', '<SD>', '<END>']\n",
      "Number of added tokens:  14\n",
      "Versions of the word 'kirk' in the vocabulary: ĠKirk, Kirk\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "script, names=preprocess_script(TOS_scripts['episode 62'])\n",
    "custom_tokens=list(names)\n",
    "for token in [\"<LOC>\", \"<CHAR>\", \"<SD>\", \"<END>\"]:\n",
    "    custom_tokens.append(token)\n",
    "new_tokens = [token for token in custom_tokens if token not in tokenizer.get_vocab()]\n",
    "print(new_tokens)\n",
    "# Add the new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Print the new vocabulary size\n",
    "print(\"Number of added tokens: \", len(new_tokens))\n",
    "\n",
    "def check_in_vocab(word_to_check):\n",
    "    word_version=[vocab_word for vocab_word in tokenizer.get_vocab() \n",
    "                  if vocab_word.lower() == word_to_check.lower() \n",
    "                  or vocab_word.lower() == (\"Ġ\" + word_to_check).lower()]\n",
    "    if word_version:\n",
    "        print(f\"Versions of the word '{word_to_check}' in the vocabulary: {', '.join(word_version)}\")\n",
    "    else:\n",
    "        print(f\"The word '{word_to_check}' is not in the vocabulary.\")\n",
    "check_in_vocab('kirk')\n",
    "# Don't forget to resize the model embeddings to match the new vocabulary size\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2c6e3-8535-4c91-a089-aa1f4ec38ccb",
   "metadata": {},
   "source": [
    "A problem I've discovered: I can add ĠKirk to the vocabulary, but the pretrained embeddings would prefer to use \"ĠK\", \"irk\". I might want to consider allowing the embeddings to be trainable, since GPT's embeddings have been trained on a general corpus, so they might not perfectly align with the specific nuances of my TV show scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85b9105-8941-4ab2-b45e-3ce90fc4606b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '<CHAR>', 'Kirk', ':', 'ĠAn', 'Ġentire', 'Ġhuman', 'Ġcolony', ',', 'Ġa', 'Ġwhole', 'Ġsettlement', '.', 'ĠOne', 'Ġhundred', 'Ġmen', ',', 'Ġwomen', 'Ġand', 'Ġchildren', '.', 'ĠWho', 'Ġdid', 'Ġit', '?', 'ĠAnd', 'Ġwhy', '?', '<SD>', 'commun', 'icator', 'Ġbe', 'ep', '<SD>', 'Kirk', 'here', '.', 'ĠSP', 'OCK', '<LOC>', 'OC', '<LOC>', ':', 'Spock', 'here', ',', 'ĠCaptain', '.', '<LOC>', 'Bridge', '<LOC>', '<CHAR>', 'Spock', ':', 'ĠSens', 'ors', 'Ġhave', 'Ġpicked', 'Ġup', 'Ġa', 'Klingon', 'ship', ',', 'Ġclosing', 'Ġfast', '.', 'ĠK', 'IR', 'K', '<LOC>', 'OC', '<LOC>', ':', 'ĠDef', 'lect', 'ors', 'Ġon', '.', 'ĠCondition', 'ĠRed', '.', 'ĠProtect', 'Ġyourselves', '.', '<LOC>', 'Planet', 'Ġsurface', '<LOC>', '<CHAR>', 'Kirk', ':', 'ĠTotal', 'Ġreply', 'Ġif', 'Ġattacked', '.', 'ĠSo', 'Ġthat', \"'s\", 'Ġthe', 'Ġanswer', '.', 'Klingon', 's', '.', '<LOC>', 'Bridge', '<LOC>', '<CHAR>', 'Sulu', ':', 'ĠTrouble', 'Ġaboard', 'Ġthe', 'Klingon', 'ship', '.', 'ĠEvidence', 'Ġof', 'Ġexplosions', ',', 'Ġmassive', 'Ġdestruction', '.', '<LOC>', 'Planet', 'Ġsurface', '<LOC>']\n",
      "<LOC> Planet surface <LOC> <SD> Kirk, McCoy, Chekov and a security guard have beamed down to a planet with a green sky and occasional outcrops of vertical rocks. Their phasers are drawn. <SD> <CHAR> Kirk: Report, Mister Chekov. <CHAR> Chekov: Full scan. Results negative. Radiation level normal. Atmosphere and terrain are undisturbed. No evidence of a colony nor any residual aftereffect of a force that might have annihilated it. <CHAR> Kirk: Life readings, Doctor McCoy? <CHAR> McCoy: Nothing. The\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize a large sample of your text\n",
    "tokens = tokenizer.tokenize(script)\n",
    "\n",
    "# Split the tokens into chunks of 128\n",
    "token_chunks = [tokens[i:i + 128] for i in range(0, len(tokens), 128)]\n",
    "\n",
    "# You may need to pad the last chunk if it's not 128 tokens long\n",
    "last_chunk = token_chunks[-1]\n",
    "if len(last_chunk) < 128:\n",
    "    last_chunk = last_chunk + [tokenizer.pad_token] * (128 - len(last_chunk))\n",
    "    token_chunks[-1] = last_chunk\n",
    "\n",
    "print(token_chunks[1])\n",
    "# # Convert chunks to input IDs\n",
    "# input_ids_chunks = [tokenizer.convert_tokens_to_ids(chunk) for chunk in token_chunks]\n",
    "# # Count the frequency of each token\n",
    "# token_counts = Counter(tokens)\n",
    "# print(script[:500])\n",
    "# print([token for token, count in token_counts.items()])\n",
    "# # Identify tokens that might be special\n",
    "# # potential_special_tokens = [token for token, count in token_counts.items() if some_condition(token, count)]\n",
    "print(script[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b8673-a4fc-4daa-8d30-7d40f418721a",
   "metadata": {},
   "source": [
    "#### Some thoughts on training and test\n",
    "I want to tokenize all the scripts and make chunks of 128 tokens used to predict the next word. But if I take words 0:128, then 1:129, and so on, the data will be highly correlated. This means I can't just randomly take 20\\% of these chunks out for test data. Instead, I'll split at the episode level. I have 80 episodes- I'll arbitrarily assign 16 episodes to test data, and set those scripts aside.\n",
    "\n",
    "### Process scripts and chunk into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55f7bc6-018d-4d6a-bc67-662495e6680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (333099, 128)\n",
      "val shape:  (46081, 128)\n",
      "test shape:  (92269, 128)\n"
     ]
    }
   ],
   "source": [
    "scripts=[TOS_scripts['episode '+str(i)] for i in range(len(TOS_scripts))]\n",
    "random_state=42\n",
    "train_scripts, test_scripts = train_test_split(scripts, test_size=0.2, random_state=random_state)\n",
    "train_scripts, val_scripts = train_test_split(train_scripts, test_size=1/8, random_state=random_state)  # 10% of 80% = 1/8\n",
    "\n",
    "# Initialize a set to hold unique new tokens\n",
    "unique_new_tokens = set()\n",
    "\n",
    "# Preprocess the scripts and collect new tokens\n",
    "processed_scripts = []\n",
    "for script in train_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_scripts.append(processed_text)\n",
    "    unique_new_tokens.update(new_tokens)\n",
    "new_tokens=list(unique_new_tokens)\n",
    "for token in [\"<LOC>\", \"<CHAR>\", \"<SD>\", \"<END>\"]:\n",
    "    new_tokens.append(token)\n",
    "add_tokens = [token for token in new_tokens if token not in tokenizer.get_vocab()]\n",
    "# Add unique new tokens to the tokenizer\n",
    "tokenizer.add_tokens(list(add_tokens))\n",
    "\n",
    "# # Tokenize all the processed scripts\n",
    "tokenized_scripts = [tokenizer.tokenize(script) for script in processed_scripts]\n",
    "\n",
    "def create_chunks(tokenized_scripts, chunk_size=129, stride=2,padded_chunks_per_script=2000):\n",
    "    #create overlapping chunks of 128 tokens to predict the next word\n",
    "    chunks = []\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    for tokenized_script in tokenized_scripts:\n",
    "        for i in range(0, len(tokenized_script) - chunk_size + 1, stride):\n",
    "            chunk = tokenized_script[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        #The user might not provide full 128 words, so lets augment using random padded sequences.\n",
    "        for _ in range(padded_chunks_per_script):\n",
    "            start_index = randint(0, len(tokenized_script) - 2) # -2 to leave room for at least one token\n",
    "            random_length = randint(1, chunk_size - 1) # Choose a random length less than chunk_size\n",
    "            end_index = start_index + random_length\n",
    "            # Select the random chunk\n",
    "            chunk = tokenized_script[start_index:end_index]\n",
    "            # Pad the chunk to the desired length\n",
    "            padding_needed = chunk_size - len(chunk)\n",
    "            pad_token = tokenizer.pad_token_id # or whatever your padding token is\n",
    "            chunk += [pad_token] * padding_needed\n",
    "\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "train_chunks=create_chunks(tokenized_scripts)\n",
    "train_X=np.array([tokenizer.convert_tokens_to_ids(chunk[:-1]) for chunk in train_chunks])\n",
    "train_y=np.array([tokenizer.convert_tokens_to_ids(chunk[1:]) for chunk in train_chunks])\n",
    "\n",
    "processed_val_scripts=[]\n",
    "for script in val_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_val_scripts.append(processed_text)\n",
    "tokenized_val_scripts = [tokenizer.tokenize(script) for script in processed_val_scripts]\n",
    "val_chunks=create_chunks(tokenized_val_scripts)\n",
    "val_X=np.array([tokenizer.convert_tokens_to_ids(chunk[:-1]) for chunk in val_chunks])\n",
    "val_y=np.array([tokenizer.convert_tokens_to_ids(chunk[1:]) for chunk in val_chunks])\n",
    "\n",
    "processed_test_scripts=[]\n",
    "for script in test_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_test_scripts.append(processed_text)\n",
    "tokenized_test_scripts = [tokenizer.tokenize(script) for script in processed_test_scripts]\n",
    "test_chunks=create_chunks(tokenized_test_scripts)\n",
    "test_X=np.array([tokenizer.convert_tokens_to_ids(chunk[:-1]) for chunk in test_chunks])\n",
    "test_y=np.array([tokenizer.convert_tokens_to_ids(chunk[1:]) for chunk in test_chunks])\n",
    "print('train shape: ',np.shape(train_X))\n",
    "print('val shape: ',np.shape(val_X))\n",
    "print('test shape: ',np.shape(test_X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf100f0e-7848-4ba8-88ca-126a152b1af5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3574  3264  4058    11   356   821 10868   510   257  6264 17087  6737\n",
      "    11   262   869  7475   286   257  8837   543   468   587  4814   329\n",
      "   625   734 10675    13  7731  1194  3668  4074  1752 12774   503   286\n",
      "   262 16161   355   356 14765   284   466    30  1867  3022   284   340\n",
      "   503   612    30  1148   428   617  6509   484  1053  1364  2157    30\n",
      " 50267    33  3796   278  2119 50267 50268 50265    25  3406  1445    11\n",
      "  8599    13 50268 50260    25   775   815   423 29842   416   783    13\n",
      "   383 10290   531   484  1549   869    13 50268 50265    25   314  1183\n",
      "   423   345  2198    76   515   534  1306  1445    13 50268 50260    25\n",
      "  8192   314  1683  4750   345   711   257   845 42010   983   286 19780\n",
      "    11 43438 50265    30 50268 50265    25  5686]\n",
      "[ 3264  4058    11   356   821 10868   510   257  6264 17087  6737    11\n",
      "   262   869  7475   286   257  8837   543   468   587  4814   329   625\n",
      "   734 10675    13  7731  1194  3668  4074  1752 12774   503   286   262\n",
      " 16161   355   356 14765   284   466    30  1867  3022   284   340   503\n",
      "   612    30  1148   428   617  6509   484  1053  1364  2157    30 50267\n",
      "    33  3796   278  2119 50267 50268 50265    25  3406  1445    11  8599\n",
      "    13 50268 50260    25   775   815   423 29842   416   783    13   383\n",
      " 10290   531   484  1549   869    13 50268 50265    25   314  1183   423\n",
      "   345  2198    76   515   534  1306  1445    13 50268 50260    25  8192\n",
      "   314  1683  4750   345   711   257   845 42010   983   286 19780    11\n",
      " 43438 50265    30 50268 50265    25  5686   799]\n"
     ]
    }
   ],
   "source": [
    "print(train_X[8])\n",
    "print(train_y[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47205eb0-798c-484e-b2e5-7ddd4ac3abbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 11:02:43.988321: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2023-08-23 11:02:43.988345: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-08-23 11:02:43.988350: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-08-23 11:02:43.988377: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-08-23 11:02:43.988389: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.layers.core.embedding.Embedding at 0x2cbff42d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Convert your tokenized data into TensorFlow tensors\n",
    "# X_train_tensor = tf.convert_to_tensor(train_X, dtype=tf.int32)\n",
    "# y_train_tensor = tf.convert_to_tensor(train_y, dtype=tf.int32)\n",
    "\n",
    "# # Create a tf.data.Dataset from the tensors\n",
    "# train_dataset = tf.data.Dataset.from_tensor_slices((X_train_tensor, y_train_tensor))\n",
    "\n",
    "# # Shuffle and batch the dataset\n",
    "# batch_size = 32\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=len(train_X)).batch(batch_size)\n",
    "\n",
    "#use GPT-2 model. We'll need to train from here to learn star trek vocab\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa59a5ac-28c4-4188-8973-f9ce7c11a776",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLay  multiple                  124649472 \n",
      " er)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124649472 (475.50 MB)\n",
      "Trainable params: 39595008 (151.04 MB)\n",
      "Non-trainable params: 85054464 (324.46 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 11:02:50.192776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1792/20819 [=>............................] - ETA: 14:33:36 - loss: 21.6283"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# # Save the weights\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# model.save_weights('embedding_only_weights.h5')\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# # Save the final model\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# model.save('final_model.h5')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training\n",
    "# Train only the embedding layer\n",
    "\n",
    "# Freeze all layers within the transformer\n",
    "for layer in model.transformer.h:\n",
    "    layer.trainable = False\n",
    "# Unfreeze the embedding layer (wte)\n",
    "model.transformer.wte.trainable = True\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "history = model.fit(x=train_X, y=train_y, epochs=10, batch_size=16, validation_data=(val_X, val_y))\n",
    "\n",
    "# # Save the weights\n",
    "# model.save_weights('embedding_only_weights.h5')\n",
    "\n",
    "# # Unfreeze more layers (optional)\n",
    "# for layer in model.layers[:n]:  # Unfreeze the first n layers\n",
    "#     layer.trainable = True\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', lr=0.0001) # Smaller learning rate\n",
    "# model.fit(training_data)\n",
    "\n",
    "# # Unfreeze all layers (optional)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', lr=0.00001) # Even smaller learning rate\n",
    "# model.fit(training_data)\n",
    "\n",
    "# # Save the final model\n",
    "# model.save('final_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b511c-5a3c-4a96-ab12-dff332cf01d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal",
   "language": "python",
   "name": "tf_metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
