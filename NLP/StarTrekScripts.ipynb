{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ca164953-ad9a-4288-84fd-051f579f1c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "51f1795b-d0e4-4e18-892a-c5fbcf8f1866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Star Trek Transcripts - The Cage\n",
      "\n",
      "\n",
      "\n",
      "The\n",
      "Cage\n",
      "Unaired\n",
      "pilot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [Bridge]\n",
      "\n",
      "SPOCK: Check the circuit. \n",
      "TYLER: All operating, sir. \n",
      "SPOCK: It can't be the screen then. Definitely something out there,\n",
      "Captain, headed this way. \n",
      "TYLER: It could be these meteorites. \n",
      "ONE: No, it's something else. There's still something out there. \n",
      "TYLER: It's coming at the speed of light, collision course. The\n",
      "meteorite beam has not deflected it, Captain.\n",
      "ONE: Evasive manoeuvres, sir?\n",
      "PIKE: Steady as we go.\n",
      "GARISON: It's a radio wave, sir. We're passing through an old-style\n",
      "distress signal.\n",
      "PIKE: They were keyed to cause interference and attract attention this\n",
      "way.\n",
      "GARISON: A ship in trouble making a forced landing, sir. That's it. No\n",
      "other message.\n",
      "TYLER: I have a fix. It comes from the Talos star group.\n",
      "ONE: We've no ships or Earth colonies that far out.\n",
      "SPOCK: Their call letters check with a survey expedition. SS Columbia.\n",
      "It disappeared in that region approximately eight\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/StarTrek_scripts/all_scripts_raw.json\")\n",
    "json_file = json.load(f)\n",
    "f.close()\n",
    "#start with TOS: might be more manageable\n",
    "TOS_scripts=json_file['TOS']\n",
    "print(TOS_scripts['episode 0'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467cff7-cb68-4c50-8a7c-ff2317c1888c",
   "metadata": {},
   "source": [
    "### General plan\n",
    "\n",
    "I want to generate a star trek script. The model will be some kind of transformer. The input is a series of tokens, I'll start with 128 tokens (needs padded in case the input is shorter). The output is the next word, i.e. input: [The, quick, brown], output: [quick, brown, fox]\n",
    "\n",
    "To make this, that means I need to:\n",
    "- parse the scripts, remove line breaks and things.\n",
    "\n",
    "    - Also need to remove episode title at beginning and copyright stuff at the end.\n",
    "    - Probably should add a character for stage direction, or perhaps if I keep the colons such that kirk: is distinct from kirk.\n",
    "    \n",
    "- Create segments of input tokens in batches of 128\n",
    "\n",
    "- Embed the wordings\n",
    "\n",
    "- Split train/test data\n",
    "\n",
    "- Create model and train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "329d7e53-c176-4f54-99ab-f6505102018f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LOC> Bridge <LOC> <CHAR> Spock: Check the circuit. <CHAR> Tyler: All operating, sir. <CHAR> Spock: It can't be the screen then. Definitely something out there, Captain, headed this way. <CHAR> Tyler: It could be these meteorites. <CHAR> One: No, it's something else. There's still something out there. <CHAR> Tyler: It's coming at the speed of light, collision course. The meteorite beam has not deflected it, Captain. <CHAR> One: Evasive manoeuvres, sir? <CHAR> Pike: Steady as we go. <CHAR> Garison: It's a radio wave, sir. We're passing through an old-style distress signal. <CHAR> Pike: They were keyed to cause interference and attract attention this way. <CHAR> Garison: A ship in trouble making a forced landing, sir. That's it. No other message. <CHAR> Tyler: I have a fix. It comes from the Talos star group. <CHAR> One: We've no ships or Earth colonies that far out. <CHAR> Spock: Their call letters check with a survey expedition. SS Columbia. It disappeared in that region approximately \n",
      "{'ĠSpock', 'ĠVina', 'ĠGeologist', 'ĠOne', 'ĠOrion', 'ĠTalosian', 'ĠGarison', 'ĠPike', 'ĠPitcairn', 'ĠMagistrate', 'ĠOld', 'ĠColt', 'ĠBoyce', 'ĠHaskins', 'ĠOfficer', 'ĠTyler', 'ĠSurvivor'}\n",
      "Captain's log, Stardate 1513.1. Our position, orbiting planet M-113. On board the Enterprise, Mister Spock temporarily in command. On the planet the ruins of an ancient and long-dead civilisation. Ship's surgeon McCoy and myself are now beaming down to the planet's surface. Our mission, routine medical examination of archaeologist Robert Crater and his wife Nancy. Routine but for the fact that Nancy Crater is that one woman in Doctor McCoy's past. <LOC> Planet surface outside building <LOC> <CHAR> Kirk: Shall we pick some flowers, Doctor? When a man visits an old girlfriend she usually expects something like that. <CHAR> McCoy: Is that how you get girls to like you, by bribing them? There doesn't seem to be anybody around, does there. <CHAR> Kirk: They'll be along. You rushed us down ten minutes early. <SD> enter building <SD> <LOC> Crater's home <LOC> <CHAR> Kirk: Professor Crater? Professor? Mrs. Crater? Nervous, Dr. McCoy? <CHAR> McCoy: Yeah, a little bit, I guess. You see, we walke\n",
      "{'ĠSpock', 'ĠMcCoy', 'ĠGreen', 'ĠTransporter', 'ĠBlueshirt', 'ĠDarnell', 'ĠRand', 'ĠSecurity', 'ĠKirk', 'ĠCrewman', 'ĠUhura', 'ĠSulu', 'ĠRedshirt', 'ĠCrater', 'ĠNancy', 'ĠBlonde'}\n",
      "Captain's Log, star date 1533.6. Now manoeuvring to come alongside cargo vessel Antares. Its Captain and First officer are beaming over to us with an unusual passenger. <LOC> Transporter room <LOC> <CHAR> Kirk: All right, Chief, begin materialisation. Captain Ramart, I'm Captain Kirk. <CHAR> Ramart: This is my navigator Tom Nellis. <CHAR> Kirk: How do you do? <CHAR> Nellis: How do you do? <CHAR> Ramart: And this is our young castaway Charlie, Charlie Evans. His dossier. <CHAR> Kirk: Mister Evans. We've heard a great deal about you. Welcome aboard. <CHAR> Ramart: Wonderful boy, Charlie. Its been an honour having him aboard. <CHAR> Nellis: Why, it's been a great pleasure. The things that he's learned in the last <CHAR> Ramart: Absolutely. To think this boy spent practically his whole life alone on that planet. Everyone killed, just a few microtapes to learn from. <CHAR> Charlie: How many humans like me on this ship? <CHAR> Ramart: Like a whole city in space, Charlie. Over four hundred in\n",
      "{'ĠSpock', 'ĠNellis', 'ĠCharlie', 'ĠPilot', 'ĠMcCoy', 'ĠSam', 'ĠNavigator', 'ĠThasian', 'ĠRand', 'ĠRamart', 'ĠTina', 'ĠKirk', 'ĠUhura'}\n",
      "Captain's log, Star date 1312.4. The impossible has happened. From directly ahead, we're picking up a recorded distress signal, the call letters of a vessel which has been missing for over two centuries. Did another Earth ship once probe out of the galaxy as we intend to do? What happened to it out there? Is this some warning they've left behind? <LOC> Briefing room <LOC> <CHAR> Spock: Your move, Captain. <CHAR> Kirk: We should have intercepted by now. The Bridge said they'd call. <CHAR> Spock: I'll have you checkmated your next move. <CHAR> Kirk: Have I ever mentioned you play a very irritating game of chess, Mister Spock? <CHAR> Spock: Irritating? Ah, yes. One of your Earth emotions. <CHAR> Kirk: Certain you don't know what irritation is? <CHAR> Spock: The fact one of my ancestors married a human female <CHAR> Kirk: Terrible having bad blood like that. KELSO <LOC> on monitor <LOC> Bridge to briefing lounge. Object is now within tractor beam range. <CHAR> Kirk: No visual contact, Mist\n",
      "{'ĠSpock', 'ĠDehner', 'ĠMitchell', 'ĠSmith', 'ĠPiper', 'ĠKirk', 'ĠKelso', 'ĠSulu', 'ĠAlden', 'ĠScott'}\n",
      "Captain's Log. Our position, orbiting Psi 2000, an ancient world, now a frozen wasteland, about to rip apart in its death throes. Our mission, pick up a scientific party below, observe the disintegration of the planet. <LOC> Psi 2000 control room <LOC> <CHAR> Spock: Check out the life-support systems. <CHAR> Tormolen: Right, sir. All life systems were off, sir. <CHAR> Spock: Someone strangled this woman. TORMOLEN The other four are back there. <CHAR> Spock: Dead? <CHAR> Tormolen: Right, sir. <CHAR> Spock: Engineer at his post? TORMOLEN He's frozen there like he didn't care. <CHAR> Spock: The rest? <CHAR> Tormolen: Well, better look for yourself, Mister Spock. One man was taking a shower fully clothed. <SD> they split up, he takes off his glove to scratch his nose then puts it on the console. Something orange jumps onto his skin. Feeling the cold he puts the glove back on <SD> <CHAR> Spock: Be certain we expose ourselves to nothing. Spock here. Do you read, Enterprise? KIRK <LOC> OC <LO\n",
      "{'ĠSpock', 'ĠMoody', 'ĠMcCoy', 'ĠBrent', 'ĠRand', 'ĠTormolen', 'ĠKirk', 'ĠChapel', 'ĠRiley', 'ĠUhura', 'ĠSulu', 'ĠScott'}\n",
      "<LOC> Planet surface <LOC> <CHAR> Kirk: That should make a good specimen. <CHAR> Sulu: <SD> holding pink animal with a horn <SD> Temperature's starting to drop. <CHAR> Kirk: Yeah. At night it gets down to a hundred and twenty degrees below zero. <CHAR> Sulu: That's nippy.  <CHAR> Fisher: Hey! <CHAR> Kirk: What happened? <CHAR> Fisher: I fell off that bank, sir. Cut my hand. <CHAR> Kirk: Let's see it. Get back to the ship. Report to the Sickbay. <CHAR> Fisher: Yes, sir. Geological Technician Fisher. Ready to beam up. <LOC> Transporter room <LOC> <CHAR> Scott: Right. Locked onto you. Energise. Coadjutor engagement. <CHAR> Wilson: What happened? <CHAR> Fisher: I took a flop. <CHAR> Wilson: Onto what? <CHAR> Fisher: I don't know. Some kind of yellow ore. <CHAR> Scott: Magnetic. Decontaminate that uniform. <CHAR> Fisher: Yes, sir. <SD> ;eaves <SD> <CHAR> Scott: That acted like a burnout. KIRK <LOC> OC <LOC>: Captain Kirk ready to beam up. <CHAR> Scott: Just one moment, Captain. It checks ou\n",
      "{'ĠSpock', 'ĠMcCoy', 'ĠRand', 'ĠKirk', 'ĠWilson', 'ĠOther', 'ĠSulu', 'ĠScott', 'ĠFarrell', 'ĠFisher'}\n",
      "Captain's log, Stardate 1329.8. The U.S.S. Enterprise in pursuit of an unidentified vessel. <LOC> Bridge <LOC> <CHAR> Sulu: There he is, sir. Centre screen. Still trying to run away from us, sir. <CHAR> Kirk: Don't lose him, Mister Sulu. <CHAR> Sulu: No, sir. <CHAR> Kirk: Earth ship, Mister Spock? <CHAR> Spock: Difficult to say, Captain. We're getting no registration beam from it. <CHAR> Scott: If it is, he'll soon overload his engines. <CHAR> Spock: Sir, he's pushing his engines too hard. <CHAR> Sulu: Changing course again. He knows we're after him all right. <CHAR> Kirk: Stay with him, Mister Sulu. Communication? <CHAR> Uhura: I've tried all frequencies, sir. He refuses to answer. Unless he's not receiving us. <CHAR> Kirk: He's receiving us alright. <CHAR> Spock: Approaching an asteroid belt, Captain. Schiller rating three five. <CHAR> Kirk: Deflectors on, Mister Farrell. <CHAR> Sulu: He's seen the asteroids too, sir. <CHAR> Kirk: Stay with him. He'll try to lose us in them. <CHAR> S\n",
      "{'ĠNudd', 'ĠMudd', 'ĠKirk', 'ĠMadga', 'ĠSpock', 'ĠBenton', 'ĠRuth', 'ĠSecurity', 'ĠUhura', 'ĠChildress', 'ĠMagda', 'ĠMcCoy', 'ĠComputer', 'ĠEve', 'ĠWomen', 'ĠSulu', 'ĠGossett', 'ĠScott', 'ĠFarrell'}\n",
      "<LOC> Bridge <LOC> CREWMAN <LOC> OC <LOC>: All sections, security check in progress. Report. Forward phaser <CHAR> Kirk: We're entering standard orbit, Nurse. It won't be much longer. CREWMAN <LOC> OC <LOC>: Forward scanner to Bridge. <CHAR> Kirk: I understand you gave up a career in bio-research to sign aboard a starship. <CHAR> Chapel: I know he's alive down there, Captain. CREWMAN 2 <LOC> OC <LOC>: Aft scanner to Bridge, Status report, please. Engineering controls <CHAR> Kirk: It's been five years since his last message. <CHAR> Chapel: Roger's a very determined man. He'd find a way to live. <CHAR> Uhura: Beginning signals to surface, sir. <CHAR> Kirk: Put it on all frequencies, Lieutenant. <CHAR> Spock: Ship's record banks show little we don't already know about this planet, Captain. Gravity is one point one of Earth, atmosphere within safety limits. <CHAR> Kirk: But the surface temperature of that planet is one hundred degrees below zero. <CHAR> Spock: It may have been inhabited on\n",
      "{'ĠSpock', 'ĠAndrea', 'ĠRuk', 'ĠBrown', 'ĠKirk', 'ĠChapel', 'ĠRayburn', 'ĠMatthews', 'ĠUhura', 'ĠKorby'}\n",
      "<LOC> Bridge <LOC> <CHAR> Kirk: Earth-style distress signal. SOS. <CHAR> Farrell: I've answered it on all frequencies, sir. They don't reply. <CHAR> Spock: Not a vessel, a ground source. The third planet in this solar system, according to my instruments. <CHAR> Farrell: Directly ahead. Definitely an Earth-style signal. <CHAR> Kirk: We're hundreds of light years from Earth, Mister Spock. No colonies or vessels out this far. <CHAR> Spock: Measuring the planet now, Captain. It's spheroid-shaped, circumference twenty four thousand eight hundred seventy four miles. Mass six times ten to the twenty first power tons. Mean density five point five one seven. Atmosphere oxygen, nitrogen. <CHAR> Rand: Earth! <CHAR> Kirk: Not the Earth, another Earth. Another Earth? Captain's Log, stardate 2713.5. In the distant reaches of our galaxy, we have made an astonishing discovery. Earth type radio signals coming from a planet which apparently is an exact duplicate of the Earth. It seems impossible, but th\n",
      "{'ĠSpock', 'ĠRed', 'ĠBlonde', 'ĠMcCoy', 'ĠTeenager', 'ĠGalloway', 'ĠJahn', 'ĠRand', 'ĠMasked', 'ĠKirk', 'ĠChildren', 'ĠFarrell', 'ĠMiri'}\n",
      "<LOC> Transporter room <LOC> <CHAR> Berkeley: Ready to beam down. Energise. Energise. <CHAR> Kirk: Having trouble, gentlemen? <CHAR> Berkeley: I just don't understand the problem, sir. <CHAR> Kirk: You're beaming cargo down to a penal colony, Mister Berkeley. <CHAR> Berkeley: Their security force field, sir. <CHAR> Kirk: U.S.S. Enterprise to Tantalus colony. WOMAN <LOC> OC <LOC>: Rehab colony. Come in. <CHAR> Kirk: Request opening in your force field for beaming down of cargo. WOMAN <LOC> OC <LOC>: Enterprise, affirmative. Our security cover is now open. <CHAR> Kirk: Energise. Any incoming cargo? <CHAR> Berkeley: Just one item, sir. Some research material bound for the Central Bureau of Penology at Stockholm. WOMAN <LOC> OC <LOC>: Tantalus cargo ready to beam up. <CHAR> Kirk: Oh, Mister Berkeley, you might refamiliarise yourself with the manual on penal colony procedures. <CHAR> Berkeley: Immediately, sir. <CHAR> Kirk: I think you can take the time to lock this up first. <CHAR> Berkele\n",
      "{'ĠSpock', 'ĠMcCoy', 'ĠGelder', 'ĠAdams', 'ĠLethe', 'ĠSecurity', 'ĠTherapist', 'ĠKirk', 'ĠBerkeley', 'ĠNoel', 'ĠUhura'}\n"
     ]
    }
   ],
   "source": [
    "#functions to remove metadata and add stage direction tokens to script\n",
    "\n",
    "def add_special_tokens(script):\n",
    "    # Replace character names\n",
    "    script = re.sub(r'\\n([A-Z ]+):', r' <CHAR> \\1:', script) #adds <CHAR> token any time theres a new line followed by \"<CAPITALLETTERS>:\"\n",
    "    # You can add more substitutions here for stage directions or other special tokens\n",
    "    script = re.sub(r'[\\[\\{]([^\\]\\}]+)[\\]\\}]', r' <LOC> \\1 <LOC>', script) #add <LOC> token to indicate location\n",
    "    script = re.sub(r'\\(([^)]+)\\)', r' <SD> \\1 <SD>', script)\n",
    "    return script\n",
    "\n",
    "def remove_metadata(script):\n",
    "    # Find the position of the 17th newline character\n",
    "    start_pos = -1\n",
    "    for _ in range(17):\n",
    "        start_pos = script.find('\\n', start_pos + 1)\n",
    "        \n",
    "    # Slice the string from the character after the 8th newline\n",
    "    if start_pos != -1:\n",
    "        script = script[start_pos + 1:]\n",
    "    \n",
    "    # Find the position of \"<Back\"\n",
    "    pos = script.find(\"<Back\")\n",
    "\n",
    "    # If found, cut off everything past that point\n",
    "    if pos != -1:\n",
    "        script = script[:pos]\n",
    "    return script\n",
    "\n",
    "def process_names(text):\n",
    "    unique_names = set()\n",
    "\n",
    "    # Function to replace \"<CHAR> NAME:\" with \"<CHAR> Name:\"\n",
    "    def char_replacer(match):\n",
    "        name = match.group(1)\n",
    "        if name.lower() == \"mccoy\":\n",
    "            name = \"McCoy\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        unique_names.add(f\"Ġ{name}\")\n",
    "        return f\"<CHAR> {name}\"\n",
    "    \n",
    "    # Replace names after \"<CHAR>\"\n",
    "    text = re.sub(r'<CHAR>\\s+([A-Z]{2,})', char_replacer, text)\n",
    "    # Function to replace all other instances of unique names\n",
    "    def name_replacer(match):\n",
    "        name = match.group(0)\n",
    "        if name == \"MCCOY\":\n",
    "            return \" McCoy\"\n",
    "        return name.capitalize() if name.upper() in unique_names else name\n",
    "\n",
    "    # Replace all other instances of unique names\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\b', name_replacer, text)\n",
    "\n",
    "    return text, unique_names\n",
    "\n",
    "def preprocess_script(script):\n",
    "    \n",
    "    script=add_special_tokens(remove_metadata(script))\n",
    "    script, names =process_names(script)\n",
    "    script=script.replace('\\n', ' ')\n",
    "    script=script.replace('\\r', ' ')\n",
    "    # Replace multiple spaces with a single space\n",
    "    script = re.sub(' +', ' ', script)\n",
    "    script += \"<END>\"\n",
    "    script = script.strip()\n",
    "    return script, names\n",
    "\n",
    "for i in range(10):\n",
    "    # print(TOS_scripts['episode '+str(i)][:1100])\n",
    "    \n",
    "    script, names=preprocess_script(TOS_scripts['episode '+str(i)])\n",
    "    print(script[:1000])\n",
    "    print(names)\n",
    "    # print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c537d614-c5d9-42dd-8d35-3fd7873966c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠChekov', 'ĠUhura', 'ĠSulu', '<LOC>', '<CHAR>', '<SD>', '<END>']\n",
      "Number of added tokens:  7\n",
      "Versions of the word 'klingon' in the vocabulary: ĠKlingon\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "script, names=preprocess_script(TOS_scripts['episode 62'])\n",
    "custom_tokens=list(names)\n",
    "for token in [\"<LOC>\", \"<CHAR>\", \"<SD>\", \"<END>\"]:\n",
    "    custom_tokens.append(token)\n",
    "new_tokens = [token for token in custom_tokens if token not in tokenizer.get_vocab()]\n",
    "print(new_tokens)\n",
    "# Add the new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Print the new vocabulary size\n",
    "print(\"Number of added tokens: \", len(new_tokens))\n",
    "\n",
    "def check_in_vocab(word_to_check):\n",
    "    word_version=[vocab_word for vocab_word in tokenizer.get_vocab() \n",
    "                  if vocab_word.lower() == word_to_check.lower() \n",
    "                  or vocab_word.lower() == (\"Ġ\" + word_to_check).lower()]\n",
    "    if word_version:\n",
    "        print(f\"Versions of the word '{word_to_check}' in the vocabulary: {', '.join(word_version)}\")\n",
    "    else:\n",
    "        print(f\"The word '{word_to_check}' is not in the vocabulary.\")\n",
    "check_in_vocab('klingon')\n",
    "# Don't forget to resize the model embeddings to match the new vocabulary size\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2c6e3-8535-4c91-a089-aa1f4ec38ccb",
   "metadata": {},
   "source": [
    "A problem I've discovered: I can add ĠKirk to the vocabulary, but the pretrained embeddings would prefer to use \"ĠK\", \"irk\". I might want to consider allowing the embeddings to be trainable, since GPT's embeddings have been trained on a general corpus, so they might not perfectly align with the specific nuances of my TV show scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e85b9105-8941-4ab2-b45e-3ce90fc4606b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠWhich', 'Ġwe', 'Ġwere', 'Ġunable', 'Ġto', 'Ġdetect', 'Ġupon', 'Ġapproach', '.', '<CHAR>', 'K', 'irk', ':', 'ĠAn', 'Ġentire', 'Ġhuman', 'Ġcolony', ',', 'Ġa', 'Ġwhole', 'Ġsettlement', '.', 'ĠOne', 'Ġhundred', 'Ġmen', ',', 'Ġwomen', 'Ġand', 'Ġchildren', '.', 'ĠWho', 'Ġdid', 'Ġit', '?', 'ĠAnd', 'Ġwhy', '?', '<SD>', 'commun', 'icator', 'Ġbe', 'ep', '<SD>', 'K', 'irk', 'Ġhere', '.', 'ĠSP', 'OCK', '<LOC>', 'OC', '<LOC>', ':', 'ĠSpock', 'Ġhere', ',', 'ĠCaptain', '.', '<LOC>', 'Bridge', '<LOC>', '<CHAR>', 'Sp', 'ock', ':', 'ĠSens', 'ors', 'Ġhave', 'Ġpicked', 'Ġup', 'Ġa', 'ĠKlingon', 'Ġship', ',', 'Ġclosing', 'Ġfast', '.', 'ĠK', 'IR', 'K', '<LOC>', 'OC', '<LOC>', ':', 'ĠDef', 'lect', 'ors', 'Ġon', '.', 'ĠCondition', 'ĠRed', '.', 'ĠProtect', 'Ġyourselves', '.', '<LOC>', 'Planet', 'Ġsurface', '<LOC>', '<CHAR>', 'K', 'irk', ':', 'ĠTotal', 'Ġreply', 'Ġif', 'Ġattacked', '.', 'ĠSo', 'Ġthat', \"'s\", 'Ġthe', 'Ġanswer', '.', 'ĠKling', 'ons', '.', '<LOC>', 'Bridge', '<LOC>', '<CHAR>', 'S', 'ulu', ':', 'ĠTrouble', 'Ġaboard', 'Ġthe', 'ĠKlingon']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize a large sample of your text\n",
    "tokens = tokenizer.tokenize(script)\n",
    "\n",
    "# Split the tokens into chunks of 128\n",
    "token_chunks = [tokens[i:i + 128] for i in range(0, len(tokens), 128)]\n",
    "\n",
    "# You may need to pad the last chunk if it's not 128 tokens long\n",
    "last_chunk = token_chunks[-1]\n",
    "if len(last_chunk) < 128:\n",
    "    last_chunk = last_chunk + [tokenizer.pad_token] * (128 - len(last_chunk))\n",
    "    token_chunks[-1] = last_chunk\n",
    "\n",
    "print(token_chunks[1])\n",
    "# # Convert chunks to input IDs\n",
    "# input_ids_chunks = [tokenizer.convert_tokens_to_ids(chunk) for chunk in token_chunks]\n",
    "# # Count the frequency of each token\n",
    "# token_counts = Counter(tokens)\n",
    "# print(script[:500])\n",
    "# print([token for token, count in token_counts.items()])\n",
    "# # Identify tokens that might be special\n",
    "# # potential_special_tokens = [token for token, count in token_counts.items() if some_condition(token, count)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b8673-a4fc-4daa-8d30-7d40f418721a",
   "metadata": {},
   "source": [
    "#### Some thoughts on training and test\n",
    "I want to tokenize all the scripts and make chunks of 128 tokens used to predict the next word. But if I take words 0:128, then 1:129, and so on, the data will be highly correlated. This means I can't just randomly take 20\\% of these chunks out for test data. Instead, I'll split at the episode level. I have 80 episodes- I'll arbitrarily assign 16 episodes to test data, and set those scripts aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "c55f7bc6-018d-4d6a-bc67-662495e6680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size:  344395\n",
      "val size:  47254\n",
      "test size:  94306\n"
     ]
    }
   ],
   "source": [
    "scripts=[TOS_scripts['episode '+str(i)] for i in range(len(TOS_scripts))]\n",
    "random_state=42\n",
    "# Assuming scripts is your original list\n",
    "train_scripts, test_scripts = train_test_split(scripts, test_size=0.2, random_state=random_state)\n",
    "train_scripts, val_scripts = train_test_split(train_scripts, test_size=1/8, random_state=random_state)  # 10% of 80% = 1/8\n",
    "\n",
    "# Initialize a set to hold unique new tokens\n",
    "unique_new_tokens = set()\n",
    "\n",
    "# Preprocess the scripts and collect new tokens\n",
    "processed_scripts = []\n",
    "for script in train_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_scripts.append(processed_text)\n",
    "    unique_new_tokens.update(new_tokens)\n",
    "new_tokens=list(unique_new_tokens)\n",
    "for token in [\"<LOC>\", \"<CHAR>\", \"<SD>\", \"<END>\"]:\n",
    "    new_tokens.append(token)\n",
    "add_tokens = [token for token in new_tokens if token not in tokenizer.get_vocab()]\n",
    "# Add unique new tokens to the tokenizer\n",
    "tokenizer.add_tokens(list(add_tokens))\n",
    "\n",
    "# # Tokenize all the processed scripts\n",
    "tokenized_scripts = [tokenizer.tokenize(script) for script in processed_scripts]\n",
    "\n",
    "def create_chunks(tokenized_scripts, chunk_size=129, stride=2,padded_chunks_per_script=2000):\n",
    "    #create overlapping chunks of 128 tokens to predict the next word\n",
    "    chunks = []\n",
    "\n",
    "    for tokenized_script in tokenized_scripts:\n",
    "        for i in range(0, len(tokenized_script) - chunk_size + 1, stride):\n",
    "            chunk = tokenized_script[i:i + chunk_size]\n",
    "            chunks.append(chunk)\n",
    "        #The user might not provide full 128 words, so lets augment using random padded sequences.\n",
    "        for _ in range(padded_chunks_per_script):\n",
    "            start_index = randint(0, len(tokenized_script) - 2) # -2 to leave room for at least one token\n",
    "            random_length = randint(1, chunk_size - 1) # Choose a random length less than chunk_size\n",
    "            end_index = start_index + random_length\n",
    "            # Select the random chunk\n",
    "            chunk = tokenized_script[start_index:end_index]\n",
    "            # Pad the chunk to the desired length\n",
    "            padding_needed = chunk_size - len(chunk)\n",
    "            pad_token = tokenizer.pad_token_id # or whatever your padding token is\n",
    "            chunk += [pad_token] * padding_needed\n",
    "\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "train_chunks=create_chunks(tokenized_scripts)\n",
    "train_X=[tokenizer.convert_tokens_to_ids(chunk[:-1]) for chunk in train_chunks]\n",
    "train_y=[tokenizer.convert_tokens_to_ids(chunk[1:]) for chunk in train_chunks]\n",
    "\n",
    "processed_val_scripts=[]\n",
    "for script in val_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_val_scripts.append(processed_text)\n",
    "tokenized_val_scripts = [tokenizer.tokenize(script) for script in processed_val_scripts]\n",
    "val_chunks=create_chunks(tokenized_val_scripts)\n",
    "val_X=[tokenizer.convert_tokens_to_ids(chunk[:-1]) for chunk in val_chunks]\n",
    "val_y=[tokenizer.convert_tokens_to_ids(chunk[1:]) for chunk in val_chunks]\n",
    "\n",
    "processed_test_scripts=[]\n",
    "for script in test_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_test_scripts.append(processed_text)\n",
    "tokenized_test_scripts = [tokenizer.tokenize(script) for script in processed_test_scripts]\n",
    "test_chunks=create_chunks(tokenized_test_scripts)\n",
    "test_X=[tokenizer.convert_tokens_to_ids(chunk[:-1]) for chunk in test_chunks]\n",
    "test_y=[tokenizer.convert_tokens_to_ids(chunk[1:]) for chunk in test_chunks]\n",
    "print('train size: ',len(train_X))\n",
    "print('val size: ',len(val_X))\n",
    "print('test size: ',len(test_X))\n",
    "# # Convert to input IDs\n",
    "# input_ids_chunks = [tokenizer.convert_tokens_to_ids(chunk) for chunk in input_sequences]\n",
    "# target_ids_chunks = [tokenizer.convert_tokens_to_ids(chunk) for chunk in target_sequences]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "bf100f0e-7848-4ba8-88ca-126a152b1af5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50260, 50262, 42, 14232, 468, 655, 19952, 510, 257, 18002, 284, 428, 6203, 618, 28901, 30151, 262, 14830, 349, 2135, 13, 50262, 50261, 30464, 726, 25, 3966, 11, 8599, 13, 11853, 257, 5664, 30, 50261, 42, 14232, 25, 317, 5664, 13, 50261, 30464, 726, 25, 632, 338, 37945, 13, 8192, 345, 6810, 1997, 6283, 546, 683, 30, 50261, 42, 14232, 25, 1400, 11, 2147, 287, 1948, 13, 4162, 5633, 50261, 30464, 726, 25, 3894, 11, 340, 338, 2147, 314, 460, 30534, 1231, 281, 12452, 11, 475, 339, 338, 1716, 6481, 1334, 425, 13, 1002, 339, 547, 407, 257, 33402, 11, 314, 1549, 2048, 910, 10927, 13, 843, 329, 1194, 1517, 11, 339, 338, 14928, 2057, 13, 314, 10667, 290, 339, 5818, 470, 17065, 379, 477, 287, 1115, 1528]\n"
     ]
    }
   ],
   "source": [
    "print(test_y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59a5ac-28c4-4188-8973-f9ce7c11a776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal",
   "language": "python",
   "name": "tf_metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
