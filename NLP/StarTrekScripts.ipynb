{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7698a984-6f4d-4a47-b69c-6f9121a030b3",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this project, I want to generate star trek scripts. \n",
    "\n",
    "I originally tokenized them using the GPT2Tokenizer and tried to use TFGPT2LMHeadModel, but I found that even if I freeze everything but the embedding layers, this takes 13 hours per epoch to train on my hardware. I didn't really appreciate the size of the GPT model.\n",
    "\n",
    "Now I will simplify a bit. I'll use a lower case tokenizer for a smaller vocab and use a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca164953-ad9a-4288-84fd-051f579f1c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matt/miniconda3/envs/tf_metal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "# from transformers import TFGPT2LMHeadModel, TFGPT2Model, GPT2Tokenizer, GPT2Config\n",
    "from transformers import DistilBertTokenizer, TFDistilBertModel\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f1795b-d0e4-4e18-892a-c5fbcf8f1866",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Star Trek Transcripts - The Cage\n",
      "\n",
      "\n",
      "\n",
      "The\n",
      "Cage\n",
      "Unaired\n",
      "pilot\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [Bridge]\n",
      "\n",
      "SPOCK: Check the circuit. \n",
      "TYLER: All operating, sir. \n",
      "SPOCK: It can't be the screen then. Definitely something out there,\n",
      "Captain, headed this way. \n",
      "TYLER: It could be these meteorites. \n",
      "ONE: No, it's something else. There's still something out there. \n",
      "TYLER: It's coming at the speed of light, collision course. The\n",
      "meteorite beam has not deflected it, Captain.\n",
      "ONE: Evasive manoeuvres, sir?\n",
      "PIKE: Steady as we go.\n",
      "GARISON: It's a radio wave, sir. We're passing through an old-style\n",
      "distress signal.\n",
      "PIKE: They were keyed to cause interference and attract attention this\n",
      "way.\n",
      "GARISON: A ship in trouble making a forced landing, sir. That's it. No\n",
      "other message.\n",
      "TYLER: I have a fix. It comes from the Talos star group.\n",
      "ONE: We've no ships or Earth colonies that far out.\n",
      "SPOCK: Their call letters check with a survey expedition. SS Columbia.\n",
      "It disappeared in that region approximately eight\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data/StarTrek_scripts/all_scripts_raw.json\")\n",
    "json_file = json.load(f)\n",
    "f.close()\n",
    "#start with TOS: might be more manageable\n",
    "TOS_scripts=json_file['TOS']\n",
    "print(TOS_scripts['episode 0'][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467cff7-cb68-4c50-8a7c-ff2317c1888c",
   "metadata": {},
   "source": [
    "### General plan\n",
    "\n",
    "I want to generate a star trek script. The model will be some kind of transformer. The input is a series of tokens, I'll start with 128 tokens (needs padded in case the input is shorter). The output is the next word, i.e. input: [The, quick, brown], output: [quick, brown, fox]\n",
    "\n",
    "To make this, that means I need to:\n",
    "- parse the scripts, remove line breaks and things.\n",
    "\n",
    "    - Also need to remove episode title at beginning and copyright stuff at the end.\n",
    "    - Probably should add a character for stage direction, or perhaps if I keep the colons such that kirk: is distinct from kirk.\n",
    "    \n",
    "- Create segments of input tokens in batches of 128\n",
    "\n",
    "- Embed the wordings\n",
    "\n",
    "- Split train/test data\n",
    "\n",
    "- Create model and train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329d7e53-c176-4f54-99ab-f6505102018f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<LOC> Bridge <LOC> <CHAR> spock: Check the circuit. <CHAR> tyler: All operating, sir. <CHAR> spock: \n",
      "<class 'str'>\n",
      "{'magistrate', 'garison', 'officer', 'geologist', 'one', 'survivor', 'orion', 'boyce', 'spock', 'haskins', 'talosian', 'colt', 'tyler', 'old', 'pike', 'pitcairn', 'vina'}\n",
      "Captain's log, Stardate 1513.1. Our position, orbiting planet M-113. On board the Enterprise, Mister\n",
      "<class 'str'>\n",
      "{'crewman', 'sulu', 'crater', 'uhura', 'green', 'blueshirt', 'kirk', 'redshirt', 'nancy', 'mccoy', 'security', 'darnell', 'transporter', 'spock', 'blonde', 'rand'}\n",
      "('<CHAR> kirk: Set ship status condition red <END>', {'kirk'})\n"
     ]
    }
   ],
   "source": [
    "#functions to remove metadata and add stage direction tokens to script\n",
    "\n",
    "def add_special_tokens(script):\n",
    "    # Replace character names\n",
    "    script = re.sub(r'\\n([A-Z ]+):', r' <CHAR> \\1:', script) #adds <CHAR> token any time theres a new line followed by \"<CAPITALLETTERS>:\"\n",
    "    # You can add more substitutions here for stage directions or other special tokens\n",
    "    script = re.sub(r'[\\[\\{]([^\\]\\}]+)[\\]\\}]', r' <LOC> \\1 <LOC>', script) #add <LOC> token to indicate location\n",
    "    script = re.sub(r'\\(([^)]+)\\)', r' <SD> \\1 <SD>', script)\n",
    "    return script\n",
    "\n",
    "def remove_metadata(script):\n",
    "    # Find the position of the 17th newline character\n",
    "    start_pos = -1\n",
    "    for _ in range(17):\n",
    "        start_pos = script.find('\\n', start_pos + 1)\n",
    "        \n",
    "    # Slice the string from the character after the 8th newline\n",
    "    if start_pos != -1:\n",
    "        script = script[start_pos + 1:]\n",
    "    \n",
    "    # Find the position of \"<Back\"\n",
    "    pos = script.find(\"<Back\")\n",
    "\n",
    "    # If found, cut off everything past that point\n",
    "    if pos != -1:\n",
    "        script = script[:pos]\n",
    "    return script\n",
    "\n",
    "# def process_names(text):\n",
    "#     unique_names = set()\n",
    "\n",
    "#     # Function to replace \"<CHAR> NAME:\" with \"<CHAR> Name:\"\n",
    "#     def char_replacer(match):\n",
    "#         name = match.group(1)\n",
    "#         if name.lower() == \"mccoy\": #McCoy needs special treatment due to unique capitalization\n",
    "#             name = \"McCoy\"\n",
    "#         else:\n",
    "#             name = name.capitalize()\n",
    "#         unique_names.add(f\"{name}\")\n",
    "#         return f\"<CHAR> {name}\"\n",
    "    \n",
    "#     # Replace names after \"<CHAR>\"\n",
    "#     text = re.sub(r'<CHAR>\\s+([A-Z]{2,})', char_replacer, text)\n",
    "#     # Function to replace all other instances of unique names\n",
    "#     def name_replacer(match):\n",
    "#         name = match.group(0)\n",
    "#         if name == \"MCCOY\":\n",
    "#             return \" McCoy\"\n",
    "#         return name.capitalize() if name.upper() in unique_names else name\n",
    "\n",
    "#     # Replace all other instances of unique names\n",
    "#     text = re.sub(r'\\b[A-Z]{2,}\\b', name_replacer, text)\n",
    "\n",
    "#     return text, unique_names\n",
    "\n",
    "def process_names_lowercase(text):\n",
    "    unique_names = set()\n",
    "\n",
    "    # Function to replace \"<CHAR> NAME:\" with \"<CHAR> Name:\"\n",
    "    def char_replacer(match):\n",
    "        name = match.group(1)\n",
    "        name = name.lower()\n",
    "        unique_names.add(f\"{name}\")\n",
    "        return f\"<CHAR> {name}\"\n",
    "    \n",
    "    # Replace names after \"<CHAR>\"\n",
    "    text = re.sub(r'<CHAR>\\s+([A-Z]{2,})', char_replacer, text)\n",
    "    # Function to replace all other instances of unique names\n",
    "    def name_replacer(match):\n",
    "        name = match.group(0)\n",
    "        return name.lower() if name.upper() in unique_names else name\n",
    "    # Replace all other instances of unique names\n",
    "    text = re.sub(r'\\b[A-Z]{2,}\\b', name_replacer, text)\n",
    "    return text, unique_names\n",
    "\n",
    "def preprocess_script(script):\n",
    "    \n",
    "    script=remove_metadata(add_special_tokens(script))\n",
    "    script, names =process_names_lowercase(script)\n",
    "    script=script.replace('\\n', ' ')\n",
    "    script=script.replace('\\r', ' ')\n",
    "    # Replace multiple spaces with a single space\n",
    "    script = re.sub(' +', ' ', script)\n",
    "    script += \" <END>\"\n",
    "    script = script.strip()\n",
    "    return script, names\n",
    "\n",
    "for i in range(2):\n",
    "    # print(TOS_scripts['episode '+str(i)][:1100])\n",
    "    # print(script)\n",
    "    script, names=preprocess_script(TOS_scripts['episode '+str(i)])\n",
    "    # print(script[-1000:])\n",
    "    print(script[:100])\n",
    "    print(type(script))\n",
    "    print(names)\n",
    "    # print('\\n')\n",
    "print(preprocess_script('\\nKIRK: Set ship status condition red'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c537d614-c5d9-42dd-8d35-3fd7873966c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sulu', 'uhura', 'chekov', 'klingon', 'spock', '<LOC>', '<CHAR>', '<SD>', '<END>']\n",
      "Number of added tokens:  9\n",
      "Versions of the word 'kirk' in the vocabulary: kirk\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "script, names=preprocess_script(TOS_scripts['episode 62'])\n",
    "custom_tokens=list(names)\n",
    "for token in [\"<LOC>\", \"<CHAR>\", \"<SD>\", \"<END>\"]:\n",
    "    custom_tokens.append(token)\n",
    "new_tokens = [token for token in custom_tokens if token not in tokenizer.get_vocab()]\n",
    "print(new_tokens)\n",
    "# Add the new tokens to the tokenizer\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "# Print the new vocabulary size\n",
    "print(\"Number of added tokens: \", len(new_tokens))\n",
    "\n",
    "def check_in_vocab(word_to_check):\n",
    "    word_version=[vocab_word for vocab_word in tokenizer.get_vocab() \n",
    "                  if vocab_word.lower() == word_to_check.lower() \n",
    "                  or vocab_word.lower() == (\"Ġ\" + word_to_check).lower()]\n",
    "    if word_version:\n",
    "        print(f\"Versions of the word '{word_to_check}' in the vocabulary: {', '.join(word_version)}\")\n",
    "    else:\n",
    "        print(f\"The word '{word_to_check}' is not in the vocabulary.\")\n",
    "check_in_vocab('kirk')\n",
    "# Don't forget to resize the model embeddings to match the new vocabulary size\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f2c6e3-8535-4c91-a089-aa1f4ec38ccb",
   "metadata": {},
   "source": [
    "A problem I've discovered: I can add ĠKirk to the vocabulary, but the pretrained embeddings would prefer to use \"ĠK\", \"irk\". I might want to consider allowing the embeddings to be trainable, since GPT's embeddings have been trained on a general corpus, so they might not perfectly align with the specific nuances of my TV show scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85b9105-8941-4ab2-b45e-3ce90fc4606b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<loc>', 'planet', 'surface', '<loc>', '<sd>', 'kirk', ',', 'mccoy', ',', 'chekov', 'and', 'a', 'security', 'guard', 'have', 'beamed', 'down', 'to', 'a', 'planet', 'with', 'a', 'green', 'sky', 'and', 'occasional', 'out', '##cr', '##ops', 'of', 'vertical', 'rocks', '.', 'their', 'phase', '##rs', 'are', 'drawn', '.', '<sd>', '<char>', 'kirk', ':', 'report', ',', 'mister', 'chekov', '.', '<char>', 'chekov', ':', 'full', 'scan', '.', 'results', 'negative', '.', 'radiation', 'level', 'normal', '.', 'atmosphere', 'and', 'terrain', 'are', 'und', '##ist', '##urbed', '.', 'no', 'evidence', 'of', 'a', 'colony', 'nor', 'any', 'residual', 'after', '##ef', '##fect', 'of', 'a', 'force', 'that', 'might', 'have', 'ann', '##ih', '##ila', '##ted', 'it', '.', '<char>', 'kirk', ':', 'life', 'readings', ',', 'doctor', 'mccoy', '?', '<char>', 'mccoy', ':', 'nothing', '.', 'they', 'said', 'they', 'were', 'being', 'attacked', 'by', 'an', 'unidentified', 'ship', '.', '<char>', 'chekov', ':', 'which', 'we', 'were', 'unable', 'to', 'detect', 'upon', 'approach', '.']\n",
      "<LOC> Planet surface <LOC> <SD> Kirk, McCoy, Chekov and a security guard have beamed down to a planet with a green sky and occasional outcrops of vertical rocks. Their phasers are drawn. <SD> <CHAR> kirk: Report, Mister Chekov. <CHAR> chekov: Full scan. Results negative. Radiation level normal. Atmosphere and terrain are undisturbed. No evidence of a colony nor any residual aftereffect of a force that might have annihilated it. <CHAR> kirk: Life readings, Doctor McCoy? <CHAR> mccoy: Nothing. The\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize a large sample of your text\n",
    "tokens = tokenizer.tokenize(script)\n",
    "\n",
    "# Split the tokens into chunks of 128\n",
    "token_chunks = [tokens[i:i + 129] for i in range(0, len(tokens), 129)]\n",
    "\n",
    "# You may need to pad the last chunk if it's not 128 tokens long\n",
    "last_chunk = token_chunks[-1]\n",
    "if len(last_chunk) < 129:\n",
    "    last_chunk = last_chunk + [tokenizer.pad_token] * (129 - len(last_chunk))\n",
    "    token_chunks[-1] = last_chunk\n",
    "\n",
    "print(token_chunks[0])\n",
    "# # Convert chunks to input IDs\n",
    "# input_ids_chunks = [tokenizer.convert_tokens_to_ids(chunk) for chunk in token_chunks]\n",
    "# # Count the frequency of each token\n",
    "# token_counts = Counter(tokens)\n",
    "# print(script[:500])\n",
    "# print([token for token, count in token_counts.items()])\n",
    "# # Identify tokens that might be special\n",
    "# # potential_special_tokens = [token for token, count in token_counts.items() if some_condition(token, count)]\n",
    "print(script[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2b8673-a4fc-4daa-8d30-7d40f418721a",
   "metadata": {},
   "source": [
    "#### Some thoughts on training and test\n",
    "I want to tokenize all the scripts and make chunks of 80 tokens used to predict the next word. But if I take words 0:8, then 1:81, and so on, the data will be highly correlated. This means I can't just randomly take 20\\% of these chunks out for test data. Instead, I'll split at the episode level. I have 80 episodes- I'll arbitrarily assign 16 episodes to test data, and set those scripts aside.\n",
    "\n",
    "### Process scripts and chunk into training and test sets\n",
    "I initially ran this with a standard layout of 10% validation, 20% test data, but I found that I wasn't really using the test data any different than the validation data. The real test is at the end when I generate something new. Furthermore, I compared two cases: one where it seemed to have its best validation loss (I only trained for ~50 epochs due to hardware limitations), and one where it had the best train loss, but was overfit. The overfit result actually seems to produce more realistic scripts in the final product, so I don't think overfitting as as dangerous for this application as it would be for others. For these reasons, I'm now going to try removing the test set and keeping only 5% in validation, and increasing the size of the training sample as much as I can, starting from my \"overfit\" results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c55f7bc6-018d-4d6a-bc67-662495e6680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  30699\n",
      "chunk: ['.', 'however', ',', 'it', 'does', 'express', 'the', 'thought', '.', '<char>', 'scot', 't', ':', 'i', \"'\", 'd', 'like', 'to', 'see', 'the', 'advisor', \"'\", 's', 'face', '.', '<char>', 'spock', ':', 'you', 'will', 'have', 'that', 'opportunity', '.', '<char>', 'scot', 't', ':']\n",
      "y: [':']\n",
      "X: ['.', 'however', ',', 'it', 'does', 'express', 'the', 'thought', '.', '<char>', 'scot', 't', ':', 'i', \"'\", 'd', 'like', 'to', 'see', 'the', 'advisor', \"'\", 's', 'face', '.', '<char>', 'spock', ':', 'you', 'will', 'have', 'that', 'opportunity', '.', '<char>', 'scot', 't', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "chunk: ['##len', '##cy', '.', '<char>', 'hodin', ':', 'surely', ',', 'mister', 'spock', ',', 'you', 'do', 'not', 'intend', ',', 'i', 'hope', ',', 'to', 'create', 'a', 'dispute', 'between', 'the', 'federation', 'and', 'gideon', '?', 'spock', '<loc>', 'on', 'screen', '<loc>', ':', 'your', 'excel', '##len', '##cy', ',', 'a', 'dispute', 'is', 'the', 'far', '##thest', 'thing', 'from', 'our']\n",
      "y: ['our']\n",
      "X: ['##len', '##cy', '.', '<char>', 'hodin', ':', 'surely', ',', 'mister', 'spock', ',', 'you', 'do', 'not', 'intend', ',', 'i', 'hope', ',', 'to', 'create', 'a', 'dispute', 'between', 'the', 'federation', 'and', 'gideon', '?', 'spock', '<loc>', 'on', 'screen', '<loc>', ':', 'your', 'excel', '##len', '##cy', ',', 'a', 'dispute', 'is', 'the', 'far', '##thest', 'thing', 'from', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "train shape:  (47, 80)\n",
      "val shape:  (2, 80)\n"
     ]
    }
   ],
   "source": [
    "scripts=[TOS_scripts['episode '+str(i)] for i in range(len(TOS_scripts))]\n",
    "random_state=42\n",
    "# train_scripts, test_scripts = train_test_split(scripts, test_size=0.2, random_state=random_state)\n",
    "# train_scripts, val_scripts = train_test_split(train_scripts, test_size=1/8, random_state=random_state)  # 10% of 80% = 1/8\n",
    "train_scripts, val_scripts = train_test_split(train_scripts, test_size=0.05, random_state=random_state)  # 10% of 80% = 1/8\n",
    "\n",
    "# Initialize a set to hold unique new tokens\n",
    "unique_new_tokens = set()\n",
    "\n",
    "# Preprocess the scripts and collect new tokens\n",
    "processed_scripts = []\n",
    "for script in train_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_scripts.append(processed_text)\n",
    "    unique_new_tokens.update(new_tokens)\n",
    "new_tokens=list(unique_new_tokens)\n",
    "for token in [\"<LOC>\", \"<CHAR>\", \"<SD>\", \"<END>\", \"[PAD]\"]:\n",
    "    new_tokens.append(token)\n",
    "add_tokens = [token for token in new_tokens if token not in tokenizer.get_vocab()]\n",
    "# Add unique new tokens to the tokenizer\n",
    "tokenizer.add_tokens(list(add_tokens))\n",
    "print('vocab size: ',len(tokenizer))\n",
    "\n",
    "# # Tokenize all the processed scripts\n",
    "tokenized_scripts = [tokenizer.tokenize(script) for script in processed_scripts]\n",
    "\n",
    "\n",
    "maxlen = 80  # Max sequence size\n",
    "def create_chunks(tokenized_scripts, chunk_size=maxlen+1, stride=1,padded_chunks_per_script=1):\n",
    "    #create overlapping chunks of 80 tokens to predict the next word\n",
    "    X=[]\n",
    "    y=[]\n",
    "    tokenizer.pad_token = '[PAD]'\n",
    "    for tokenized_script in tokenized_scripts:\n",
    "        # for i in range(0, len(tokenized_script) - chunk_size + 1, stride):\n",
    "        #     chunk = tokenized_script[i:i + chunk_size]\n",
    "        #     X.append(chunk[:-1])\n",
    "        #     y.append([chunk[-1]])\n",
    "        #The user might not provide full 80 words, so lets augment using random padded sequences.\n",
    "        for _ in range(padded_chunks_per_script):\n",
    "            start_index = randint(0, len(tokenized_script) - 2) # -2 to leave room for at least one token\n",
    "            random_length = randint(2, chunk_size - 1) # Choose a random length less than chunk_size\n",
    "            end_index = start_index + random_length\n",
    "            # Select the random chunk\n",
    "            \n",
    "            chunk = tokenized_script[start_index:end_index]\n",
    "            print('chunk:',chunk)\n",
    "            y.append([chunk[-1]])\n",
    "            X_chunk=chunk[:-1]\n",
    "            print('y:',[chunk[-1]])\n",
    "            # Pad the chunk to the desired length\n",
    "            padding_needed = chunk_size - len(chunk)\n",
    "            pad_token = tokenizer.pad_token_id # or whatever your padding token is\n",
    "            X_chunk += [pad_token] * padding_needed\n",
    "            X.append(X_chunk)\n",
    "            print('X:',X_chunk)\n",
    "    X =np.array([tokenizer.convert_tokens_to_ids(x) for x in X])\n",
    "    y =np.array([tokenizer.convert_tokens_to_ids(_y) for _y in y])\n",
    "    return X,y\n",
    "\n",
    "# train_X, train_y=create_chunks(tokenized_scripts)\n",
    "\n",
    "processed_val_scripts=[]\n",
    "for script in val_scripts:\n",
    "    processed_text, new_tokens = preprocess_script(script)\n",
    "    processed_val_scripts.append(processed_text)\n",
    "tokenized_val_scripts = [tokenizer.tokenize(script) for script in processed_val_scripts]\n",
    "val_X, val_y=create_chunks(tokenized_val_scripts)\n",
    "\n",
    "# processed_test_scripts=[]\n",
    "# for script in test_scripts:\n",
    "#     processed_text, new_tokens = preprocess_script(script)\n",
    "#     processed_test_scripts.append(processed_text)\n",
    "# tokenized_test_scripts = [tokenizer.tokenize(script) for script in processed_test_scripts]\n",
    "# test_X, test_y=create_chunks(tokenized_test_scripts\n",
    "print('train shape: ',np.shape(train_X))\n",
    "print('val shape: ',np.shape(val_X))\n",
    "# print('test shape: ',np.shape(test_X))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf100f0e-7848-4ba8-88ca-126a152b1af5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example=train_X[-1]\n",
    "tokens = tokenizer.convert_ids_to_tokens(example)\n",
    "print(tokens)\n",
    "print(' '.join(tokens).replace(' ##', '')) # Joining tokens and handling subtokens\n",
    "print(tokenizer.convert_ids_to_tokens(train_y[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f6a3c-bad9-46e3-b56d-7ba2df9808c6",
   "metadata": {},
   "source": [
    "### Define miniature transformer model\n",
    "\n",
    "I'm adapting the small transformer from https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
    "seeing as GPT is way to big for me to train with my hardware.\n",
    "\n",
    "It looks like a big part of my parameters comes from word embeddings and the dense layer at the end, so I might want to try to reduce my vocabulary. I could use TextVectorization instead of BERT's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2868b36a-59dc-4a74-a6e3-b8a601dc8cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build a miniature transformer model based on https://keras.io/examples/generative/text_generation_with_miniature_gpt/\n",
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "    \n",
    "vocab_size = len(tokenizer)  # Only consider the top 20k words\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "feed_forward_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    # x = layers.Dense(vocab_size)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x) #added this layer to change the architecture to predict only 1 word\n",
    "    outputs = layers.Dense(vocab_size)(x) #logits of single predicted word\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\", loss=loss_fn,\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751a6d25-b4ef-4522-9ea2-687dd6874020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 80)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddi  (None, 80, 128)           3939712   \n",
      " ng_13 (TokenAndPositionEmb                                      \n",
      " edding)                                                         \n",
      "                                                                 \n",
      " transformer_block_13 (Tran  (None, 80, 128)           561024    \n",
      " sformerBlock)                                                   \n",
      "                                                                 \n",
      " global_average_pooling1d_1  (None, 128)               0         \n",
      " 3 (GlobalAveragePooling1D)                                      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 30699)             3960171   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8460907 (32.28 MB)\n",
      "Trainable params: 8460907 (32.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 08:20:09.702284: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10621/10621 [==============================] - ETA: 0s - loss: 3.7778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 08:44:55.456867: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10621/10621 [==============================] - 1574s 148ms/step - loss: 3.7778 - val_loss: 4.4486\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 1886s 178ms/step - loss: 3.6278 - val_loss: 4.4435\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 1896s 178ms/step - loss: 3.5169 - val_loss: 4.4716\n",
      "Epoch 4/10\n",
      "10621/10621 [==============================] - 1930s 182ms/step - loss: 3.4327 - val_loss: 4.4683\n",
      "Epoch 5/10\n",
      "10621/10621 [==============================] - 1986s 187ms/step - loss: 3.3685 - val_loss: 4.5540\n",
      "Epoch 6/10\n",
      "10621/10621 [==============================] - 2016s 190ms/step - loss: 3.3018 - val_loss: 4.5786\n",
      "Epoch 7/10\n",
      "10621/10621 [==============================] - 2045s 193ms/step - loss: 3.2456 - val_loss: 4.5696\n",
      "Epoch 8/10\n",
      "10621/10621 [==============================] - 2085s 196ms/step - loss: 3.1941 - val_loss: 4.6026\n",
      "Epoch 9/10\n",
      "10621/10621 [==============================] - 2060s 194ms/step - loss: 3.1412 - val_loss: 4.6465\n",
      "Epoch 10/10\n",
      "10621/10621 [==============================] - 1773s 167ms/step - loss: 3.1020 - val_loss: 4.7207\n",
      "Epoch 1/10\n",
      "10621/10621 [==============================] - 1836s 173ms/step - loss: 3.0665 - val_loss: 4.7380\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 1843s 174ms/step - loss: 3.0306 - val_loss: 4.7614\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 1933s 182ms/step - loss: 2.9954 - val_loss: 4.7719\n",
      "Epoch 4/10\n",
      "10621/10621 [==============================] - 1950s 184ms/step - loss: 2.9674 - val_loss: 4.8249\n",
      "Epoch 5/10\n",
      "10621/10621 [==============================] - 2092s 197ms/step - loss: 2.9308 - val_loss: 4.8300\n",
      "Epoch 6/10\n",
      "10621/10621 [==============================] - 2145s 202ms/step - loss: 2.9086 - val_loss: 4.8628\n",
      "Epoch 7/10\n",
      "10621/10621 [==============================] - 2182s 205ms/step - loss: 2.8865 - val_loss: 4.8581\n",
      "Epoch 8/10\n",
      "10621/10621 [==============================] - 2192s 206ms/step - loss: 2.8581 - val_loss: 4.8438\n",
      "Epoch 9/10\n",
      "10621/10621 [==============================] - 2229s 210ms/step - loss: 2.8379 - val_loss: 4.8839\n",
      "Epoch 10/10\n",
      "10621/10621 [==============================] - 2243s 211ms/step - loss: 2.8225 - val_loss: 4.9709\n",
      "Epoch 1/10\n",
      "10621/10621 [==============================] - 2269s 214ms/step - loss: 2.7966 - val_loss: 4.9497\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 2338s 220ms/step - loss: 2.7728 - val_loss: 4.9478\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 2364s 223ms/step - loss: 2.7470 - val_loss: 4.9690\n",
      "Epoch 4/10\n",
      "10621/10621 [==============================] - 2299s 216ms/step - loss: 2.7253 - val_loss: 4.9914\n",
      "Epoch 5/10\n",
      "10621/10621 [==============================] - 2046s 193ms/step - loss: 2.7061 - val_loss: 5.0299\n",
      "Epoch 6/10\n",
      "10621/10621 [==============================] - 2032s 191ms/step - loss: 2.6946 - val_loss: 5.0906\n",
      "Epoch 7/10\n",
      "10621/10621 [==============================] - 2023s 190ms/step - loss: 2.6779 - val_loss: 4.9523\n",
      "Epoch 8/10\n",
      "10621/10621 [==============================] - 2054s 193ms/step - loss: 2.6702 - val_loss: 5.0558\n",
      "Epoch 9/10\n",
      "10621/10621 [==============================] - 2050s 193ms/step - loss: 2.6410 - val_loss: 5.0109\n",
      "Epoch 10/10\n",
      "10621/10621 [==============================] - 2073s 195ms/step - loss: 2.6169 - val_loss: 5.0380\n",
      "Epoch 1/10\n",
      "10621/10621 [==============================] - 2091s 197ms/step - loss: 2.5983 - val_loss: 5.1587\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 2070s 195ms/step - loss: 2.5873 - val_loss: 5.1819\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 2100s 198ms/step - loss: 2.5751 - val_loss: 5.0473\n",
      "Epoch 4/10\n",
      "10621/10621 [==============================] - 2074s 195ms/step - loss: 2.5668 - val_loss: 5.2046\n",
      "Epoch 5/10\n",
      "10621/10621 [==============================] - 1963s 185ms/step - loss: 2.5506 - val_loss: 5.1867\n",
      "Epoch 6/10\n",
      "10621/10621 [==============================] - 2008s 189ms/step - loss: 2.5297 - val_loss: 5.0878\n",
      "Epoch 7/10\n",
      "10621/10621 [==============================] - 1921s 181ms/step - loss: 2.5273 - val_loss: 5.2039\n",
      "Epoch 8/10\n",
      "10621/10621 [==============================] - 1950s 184ms/step - loss: 2.4967 - val_loss: 5.1794\n",
      "Epoch 9/10\n",
      "10621/10621 [==============================] - 1897s 179ms/step - loss: 2.4917 - val_loss: 5.3098\n",
      "Epoch 10/10\n",
      "10621/10621 [==============================] - 1928s 181ms/step - loss: 2.4708 - val_loss: 5.2989\n",
      "Epoch 1/10\n",
      "10621/10621 [==============================] - 1909s 180ms/step - loss: 2.4673 - val_loss: 5.2631\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 1940s 183ms/step - loss: 2.4405 - val_loss: 5.3925\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 2034s 191ms/step - loss: 2.4403 - val_loss: 5.4237\n",
      "Epoch 4/10\n",
      "10621/10621 [==============================] - 2136s 201ms/step - loss: 2.4110 - val_loss: 5.2563\n",
      "Epoch 5/10\n",
      "10621/10621 [==============================] - 2312s 218ms/step - loss: 2.4058 - val_loss: 5.4544\n",
      "Epoch 6/10\n",
      "10621/10621 [==============================] - 2344s 221ms/step - loss: 2.3880 - val_loss: 5.2906\n",
      "Epoch 7/10\n",
      "10621/10621 [==============================] - 2373s 223ms/step - loss: 2.3804 - val_loss: 5.3251\n",
      "Epoch 8/10\n",
      "10621/10621 [==============================] - 2494s 235ms/step - loss: 2.3811 - val_loss: 5.3409\n",
      "Epoch 9/10\n",
      "10621/10621 [==============================] - 2302s 217ms/step - loss: 2.3601 - val_loss: 5.4280\n",
      "Epoch 10/10\n",
      "10621/10621 [==============================] - 2188s 206ms/step - loss: 2.3373 - val_loss: 5.3796\n",
      "Epoch 1/10\n",
      "10621/10621 [==============================] - 2217s 209ms/step - loss: 2.3180 - val_loss: 5.4013\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 2323s 219ms/step - loss: 2.3035 - val_loss: 5.4972\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 2458s 231ms/step - loss: 2.2949 - val_loss: 5.5647\n",
      "Epoch 4/10\n",
      "10621/10621 [==============================] - 2461s 232ms/step - loss: 2.2876 - val_loss: 5.5436\n",
      "Epoch 5/10\n",
      "10621/10621 [==============================] - 2469s 232ms/step - loss: 2.2719 - val_loss: 5.4746\n",
      "Epoch 6/10\n",
      "10621/10621 [==============================] - 2454s 231ms/step - loss: 2.2805 - val_loss: 5.4182\n",
      "Epoch 7/10\n",
      "10621/10621 [==============================] - 2459s 231ms/step - loss: 2.2611 - val_loss: 5.5616\n",
      "Epoch 8/10\n",
      "10621/10621 [==============================] - 2481s 234ms/step - loss: 2.2468 - val_loss: 5.6279\n",
      "Epoch 9/10\n",
      "10621/10621 [==============================] - 2471s 233ms/step - loss: 2.2508 - val_loss: 5.6903\n",
      "Epoch 10/10\n",
      "10621/10621 [==============================] - 2492s 235ms/step - loss: 2.2430 - val_loss: 5.6727\n",
      "Epoch 1/10\n",
      "10621/10621 [==============================] - 2504s 236ms/step - loss: 2.2111 - val_loss: 5.7434\n",
      "Epoch 2/10\n",
      "10621/10621 [==============================] - 2466s 232ms/step - loss: 2.2052 - val_loss: 5.6751\n",
      "Epoch 3/10\n",
      "10621/10621 [==============================] - 2472s 233ms/step - loss: 2.1949 - val_loss: 5.6642\n",
      "Epoch 4/10\n",
      " 9346/10621 [=========================>....] - ETA: 4:42 - loss: 2.1755"
     ]
    }
   ],
   "source": [
    "\n",
    "model = create_model()\n",
    "# model.load_weights('StarTrek_model_checkpoint.h5')\n",
    "model.summary()\n",
    "with open('StarTrek_oneword_history.json', 'r') as f:\n",
    "    loaded_history = json.load(f)\n",
    "\n",
    "for loop_i in range(10):\n",
    "    checkpoint_filepath = 'StarTrek_oneword_model_checkpoint'+str(loop_i+1)+'.h5'\n",
    "\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "    model.load_weights('StarTrek_oneword_model_checkpoint'+str(loop_i)+'.h5')\n",
    "    history=model.fit(train_X, \n",
    "                  train_y, \n",
    "                  validation_data=(val_X, val_y), \n",
    "                  callbacks=[model_checkpoint_callback], \n",
    "                  verbose=1, \n",
    "                  epochs=10)\n",
    "    hist_dict=history.history\n",
    "    for key in hist_dict.keys():\n",
    "        loaded_history[key].extend(hist_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "738e057d-1d4a-40e8-a4b4-12b7cf341ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/2928 [..............................] - ETA: 11:21 - loss: 7.2165"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 08:48:21.420623: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2928/2928 [==============================] - 155s 53ms/step - loss: 6.1968\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.load_weights('StarTrek_oneword_model_checkpoint8.h5')\n",
    "loss = model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2c3e417-5be7-4018-ac12-07d61685db24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#plot history\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# with open('StarTrek_oneword_history.json', 'w') as f:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     json.dump(history.history, f)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# history_dict = history.history\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_history\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m loaded_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(loss) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_history' is not defined"
     ]
    }
   ],
   "source": [
    "#plot history\n",
    "# with open('StarTrek_oneword_history.json', 'w') as f:\n",
    "#     json.dump(history.history, f)\n",
    "# history_dict = history.history\n",
    "loss = loaded_history['loss']\n",
    "val_loss = loaded_history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # Further training\n",
    "# new_history = model.fit(train_X, train_y, epochs=5)\n",
    "\n",
    "# Appending new history to initial history\n",
    "# for key in initial_hist_dict.keys():\n",
    "#     initial_hist_dict[key].extend(new_history.history[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2efd7cf0-1168-4fd5-a92e-f93cee6294d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11332  1024  1037  3371  1012 30528 16075  1024  2009  1005  1055 30526\n",
      "  1012  2031  2017  4384  2505  4326  2055  2032  1029 30528 11332  1024\n",
      "  2053  1010  2498  1999  3327  1012  2339  1029 30528 16075  1024  2092\n",
      "  1010  2009  1005  1055  2498  1045  2064  9231  8400  2302  2019  7749\n",
      "  1010  2021  2002  1005  1055  2468  6233  2717  3512  1012  2065  2002\n",
      "  2020  2025  1037 25993  1010  1045  1005  1040  2471  2360  6091  1012\n",
      "  1998  2005  2178  2518  1010  2002  1005  1055]\n",
      "['have']\n",
      "['kirk', ':', 'a', 'minute', '.', '<char>', 'mccoy', ':', 'it', \"'\", 's', 'spock', '.', 'have', 'you', 'noticed', 'anything', 'strange', 'about', 'him', '?', '<char>', 'kirk', ':', 'no', ',', 'nothing', 'in', 'particular', '.', 'why', '?', '<char>', 'mccoy', ':', 'well', ',', 'it', \"'\", 's', 'nothing', 'i', 'can', 'pin', '##point', 'without', 'an', 'examination', ',', 'but', 'he', \"'\", 's', 'become', 'increasingly', 'rest', '##ive', '.', 'if', 'he', 'were', 'not', 'a', 'vulcan', ',', 'i', \"'\", 'd', 'almost', 'say', 'nervous', '.', 'and', 'for', 'another', 'thing', ',', 'he', \"'\", 's']\n",
      "input = \n",
      " kirk : a minute . <char> mccoy : it ' s spock . have you noticed anything strange about him ? <char> kirk : no , nothing in particular . why ? <char> mccoy : well , it ' s nothing i can pinpoint without an examination , but he ' s become increasingly restive . if he were not a vulcan , i ' d almost say nervous . and for another thing , he ' s \n",
      "\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "\n",
      "\n",
      "generated = \n",
      " kirk : a minute . <char> mccoy : it ' s spock . have you noticed anything strange about him ? <char> kirk : no , nothing in particular . why ? <char> mccoy : well , it ' s nothing i can pinpoint without an examination , but he ' s become increasingly restive . if he were not a vulcan , i ' d almost say nervous . and for another thing , he ' s not\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 08:50:56.408249: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "def convert_input_to_token_idx(string):\n",
    "    processed_string=preprocess_script(string)[0][:-5]\n",
    "    input_tokens= tokenizer.tokenize(processed_string)\n",
    "    input_tokens=tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "    padding_needed = 80 - len(input_tokens)\n",
    "    pad_token = tokenizer.pad_token_id # or whatever your padding token is\n",
    "    input_tokens += [pad_token] * padding_needed\n",
    "    return np.array(input_tokens)\n",
    "\n",
    "def convert_token_idx_to_string(tokens):\n",
    "    string_array=tokenizer.convert_ids_to_tokens(tokens)\n",
    "    string_array=[string for string in string_array if string != \"[PAD]\"]\n",
    "    text = ' '.join(string_array).replace(' ##', '').replace('[PAD]', '')\n",
    "    return text\n",
    "\n",
    "input_string=\"Captain's log, star\"\n",
    "# input_string=\"\\nKIRK: Set ship status condition\"\n",
    "input_tokens=convert_input_to_token_idx(input_string)\n",
    "input_tokens=test_X[17]\n",
    "print(input_tokens)\n",
    "print(tokenizer.convert_ids_to_tokens(train_y[8]))\n",
    "# print(input_tokens)\n",
    "# print(type(train_X[0]))\n",
    "# print(np.shape(train_X[0]))\n",
    "print(tokenizer.convert_ids_to_tokens(input_tokens))\n",
    "print('input = \\n',convert_token_idx_to_string(input_tokens),'\\n')\n",
    "generated_token=np.argmax(model.predict(np.expand_dims(input_tokens,axis=0)),axis=-1)\n",
    "next_word=convert_token_idx_to_string(generated_token)\n",
    "print('\\n')\n",
    "print('generated = \\n', convert_token_idx_to_string(input_tokens)+' '+next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba347ac2-8269-4047-b5da-238651708068",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kirk : a minute . \n",
      " mccoy : it ' s spock . have you noticed anything strange about him ? \n",
      " kirk : no , nothing in particular . why ? \n",
      " mccoy : well , it ' s nothing i can pinpoint without an examination , but he ' s become increasingly restive . if he were not a vulcan , i ' d almost say nervous . and for another thing , he ' s not a heart . \n",
      " mccoy : indeed it ' s the exactment the zen planet , but you said survive there . there said there said there could say they can do it do me . do they understand me do do me which achilles achilles me which which which which which perhaps all all , and spock . <sd> they enters to kill kill you ! \n",
      " jailor : then you are you ! \n",
      " kirk : no , no ! \n",
      " kirk : your friends and you ' ll kill you ! \n",
      " kirk : no\n"
     ]
    }
   ],
   "source": [
    "#generate a longer script\n",
    "\n",
    "input_tokens=test_X[17]\n",
    "script=list(input_tokens)\n",
    "for i in range(100):\n",
    "    generated_token=list(np.argmax(model.predict(np.expand_dims(input_tokens,axis=0),verbose=0),axis=-1))\n",
    "    script.append(generated_token[0])\n",
    "    input_tokens=script[-80:]\n",
    "script=convert_token_idx_to_string(script)\n",
    "script = script.replace('<char>', '\\n')\n",
    "print(script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6f40d6-8556-4b1b-8845-74ca7fb35578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experimenting using TextVectorization tokenization instead of DistilBERT\n",
    "# scripts=[TOS_scripts['episode '+str(i)] for i in range(len(TOS_scripts))]\n",
    "# random_state=42\n",
    "# train_scripts, test_scripts = train_test_split(scripts, test_size=0.2, random_state=random_state)\n",
    "# train_scripts, val_scripts = train_test_split(train_scripts, test_size=1/8, random_state=random_state)  # 10% of 80% = 1/8\n",
    "\n",
    "# processed_train_scripts = [preprocess_script(script)[0] for script in train_scripts]\n",
    "# big_string = ' '.join(processed_train_scripts)\n",
    "\n",
    "# # Convert to a TensorFlow Dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices([big_string])\n",
    "# def custom_standardization(input_string):\n",
    "#     \"\"\" Remove html line-break tags and handle punctuation \"\"\"\n",
    "#     lowercased = tf.strings.lower(input_string)\n",
    "#     stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "#     cleaned_string = tf.strings.regex_replace(stripped_html, \"\\xa0\", \" \")  # Replacing non-breaking space with regular space\n",
    "#     # Exclude < and > from the punctuation string\n",
    "#     punctuation_without_angle_brackets = string.punctuation.replace(\"<\", \"\").replace(\">\", \"\")\n",
    "#     return tf.strings.regex_replace(cleaned_string, f\"([{punctuation_without_angle_brackets}])\", r\" \\1\")\n",
    "\n",
    "# # Create a vectorization layer and adapt it to the text\n",
    "# vectorize_layer = TextVectorization(\n",
    "#     standardize=custom_standardization,\n",
    "#     max_tokens=vocab_size - 1,\n",
    "#     output_mode=\"int\",\n",
    "#     # output_sequence_length=maxlen + 1,\n",
    "# )\n",
    "# with tf.device('/CPU:0'):\n",
    "#     # Your code here (e.g., adapting the vectorize layer)\n",
    "#     vectorize_layer.adapt(dataset) #for some reason this won't work on my GPU\n",
    "# vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "# #create chunks of training, val, and test data\n",
    "# window_size = 81\n",
    "# step_size = 3  # This defines the overlap; change as needed\n",
    "# padded_chunks_per_script=0 #not yet implemented\n",
    "\n",
    "# # Tokenize all the processed scripts\n",
    "# processed_train_scripts_tensor = tf.constant(processed_train_scripts)\n",
    "# vectorized_train_scripts = vectorize_layer(processed_train_scripts_tensor)\n",
    "# def create_chunks(tokenized_scripts, window_size=window_size, step_size=step_size, padded_chunks_per_script=padded_chunks_per_script):\n",
    "#     chunks = []\n",
    "#     for script in tokenized_scripts:\n",
    "#         for i in range(0, len(script) - window_size + 1, step_size):\n",
    "#             chunk = script[i:i + window_size]\n",
    "#             chunks.append(chunk)\n",
    "#     return chunks\n",
    "\n",
    "# train_chunks=create_chunks(vectorized_train_scripts)\n",
    "# train_X=np.array([chunk[:-1] for chunk in train_chunks])\n",
    "# train_y=np.array([chunk[1:] for chunk in train_chunks])\n",
    "\n",
    "# #do the same for validation and test data\n",
    "# processed_val_scripts = [preprocess_script(script)[0] for script in val_scripts]\n",
    "# processed_val_scripts_tensor = tf.constant(processed_val_scripts)\n",
    "# vectorized_val_scripts = vectorize_layer(processed_val_scripts_tensor)\n",
    "# val_chunks=create_chunks(vectorized_val_scripts)\n",
    "# val_X=np.array([chunk[:-1] for chunk in val_chunks])\n",
    "# val_y=np.array([chunk[1:] for chunk in val_chunks])\n",
    "\n",
    "\n",
    "# processed_test_scripts = [preprocess_script(script)[0] for script in test_scripts]\n",
    "# processed_test_scripts_tensor = tf.constant(processed_test_scripts)\n",
    "# vectorized_test_scripts = vectorize_layer(processed_test_scripts_tensor)\n",
    "# test_chunks=create_chunks(vectorized_test_scripts)\n",
    "# test_X=np.array([chunk[:-1] for chunk in test_chunks])\n",
    "# test_y=np.array([chunk[1:] for chunk in test_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1558e055-9c4e-4b77-bc9e-b8146d2ea512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #demonstrate how the vectorize layer works and how the x y vectors work\n",
    "# print(np.shape(train_X))\n",
    "# print(np.shape(val_X))\n",
    "# print(np.shape(test_X))\n",
    "# print(train_X[-1]) #batching caused shorter scripts to be filled with zeros. Maybe go back to the loop mehtod\n",
    "# print([vocab[idx] for idx in val_X[12]])\n",
    "# print([vocab[idx] for idx in val_y[12]])\n",
    "# # print(vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47205eb0-798c-484e-b2e5-7ddd4ac3abbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# #use GPT-2 model. We'll need to train from here to learn star trek vocab\n",
    "# Initializing a GPT2 configuration\n",
    "\n",
    "configuration = GPT2Config.from_pretrained('distilgpt2')\n",
    "configuration.vocab_size = len(tokenizer)\n",
    "# configuration.n_positions = 128 #The maximum sequence length that this model might ever be used with. \n",
    "# configuration.n_embd = 768, #768, #Dimensionality of the embeddings and hidden states.\n",
    "# configuration.n_layer = 4, #12, #Number of hidden layers in the Transformer encoder.\n",
    "# configuration.n_head = 4, #12, #Number of attention heads for each attention layer in the Transformer encoder.\n",
    "                           \n",
    "model = TFGPT2Model(configuration)\n",
    "model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size\n",
    "\n",
    "# If you want to see the summary, you can build the model first:\n",
    "model.build(input_shape=(None, 128))  # None for batch size, 128 for sequence length\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# configuration = GPT2Config(vocab_size = len(tokenizer), \n",
    "#                            n_positions = 128, #The maximum sequence length that this model might ever be used with. \n",
    "#                            n_embd = 64, #768, #Dimensionality of the embeddings and hidden states.\n",
    "#                            n_layer = 4, #12, #Number of hidden layers in the Transformer encoder.\n",
    "#                            n_head = 4, #12, #Number of attention heads for each attention layer in the Transformer encoder.\n",
    "#                            n_inner = None,\n",
    "#                            activation_function = 'gelu_new',\n",
    "#                            resid_pdrop = 0.1,\n",
    "#                            embd_pdrop = 0.1,\n",
    "#                            attn_pdrop = 0.1,\n",
    "#                            layer_norm_epsilon = 1e-05,\n",
    "#                            initializer_range = 0.02,\n",
    "#                            summary_type = 'cls_index',\n",
    "#                            summary_use_proj = True,\n",
    "#                            summary_activation = None,\n",
    "#                            summary_proj_to_labels = True,\n",
    "#                            summary_first_dropout = 0.1,\n",
    "#                            scale_attn_weights = True,\n",
    "#                            use_cache = True,\n",
    "#                            bos_token_id = 50256,\n",
    "#                            eos_token_id = 50256,\n",
    "#                            scale_attn_by_inverse_layer_idx = False,\n",
    "#                            reorder_and_upcast_attn = False)\n",
    "\n",
    "# # Initializing a model (with random weights) from the configuration\n",
    "# model = TFGPT2LMHeadModel(configuration)\n",
    "# # from tensorflow.keras.layers import Input, Dense\n",
    "# # from tensorflow.keras.models import Model\n",
    "# # input_layer = Input(shape=(128,), dtype='int32')\n",
    "# # gpt2_output = gpt2_model(input_layer)\n",
    "# # output_layer = Dense(vocab_size, activation='softmax')(gpt2_output[0][:,-1,:])\n",
    "\n",
    "# # model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# # model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "# model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size\n",
    "# # print(len(tokenizer))\n",
    "\n",
    "# # #use DistilBERT model. We'll need to train from here to learn star trek vocab\n",
    "# # model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "# # model.resize_token_embeddings(len(tokenizer))  # resizing to match new vocab size\n",
    "# print(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa59a5ac-28c4-4188-8973-f9ce7c11a776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training\n",
    "# Train only the embedding layer\n",
    "\n",
    "# Freeze all layers within the transformer\n",
    "for layer in model.transformer.h:\n",
    "    layer.trainable = False\n",
    "# Unfreeze the embedding layer (wte)\n",
    "model.transformer.wte.trainable = True\n",
    "# Freeze all layers first\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # Unfreeze the embedding layer\n",
    "# model.layers[0].embeddings.trainable = True\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "history = model.fit(x=train_X, y=train_y, epochs=10, batch_size=32, validation_data=(val_X, val_y))\n",
    "\n",
    "# # Save the weights\n",
    "model.save_weights('StarTrek_weights.h5')\n",
    "\n",
    "# # Unfreeze more layers (optional)\n",
    "# for layer in model.layers[:n]:  # Unfreeze the first n layers\n",
    "#     layer.trainable = True\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', lr=0.0001) # Smaller learning rate\n",
    "# model.fit(training_data)\n",
    "\n",
    "# # Unfreeze all layers (optional)\n",
    "# for layer in model.layers:\n",
    "#     layer.trainable = True\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', lr=0.00001) # Even smaller learning rate\n",
    "# model.fit(training_data)\n",
    "\n",
    "# # Save the final model\n",
    "# model.save('final_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b511c-5a3c-4a96-ab12-dff332cf01d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_loss = history.history['loss']\n",
    "# val_loss = history.history['val_loss']\n",
    "# plt.plot(train_loss, label='Training Loss')\n",
    "# plt.plot(val_loss, label='Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee74234-3418-4357-b018-59f38150918e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_metal",
   "language": "python",
   "name": "tf_metal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
